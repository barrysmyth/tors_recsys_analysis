{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca76df0-4515-405f-a74b-f7969d10b04f",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "The purpose of this notebook is to perform various check and cleanup on our refined dataset of papers and also the authors datset. The result is tw cleaned datasets that are saveed in the `processed subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be619b3d-cb3c-4431-830f-88d501ff7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import requests\n",
    "from itertools import chain\n",
    "from more_itertools import sliced\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import plt\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob, iglob\n",
    "from pathlib import Path\n",
    "                         \n",
    "from loguru import logger\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "# nltk.download('words')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import enchant\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('paper')\n",
    "\n",
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813093d2-bd7f-40db-b045-23e6f8a6343c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519c7f6-304a-40d2-a908-c069976156bc",
   "metadata": {},
   "source": [
    "## Datasets and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72167b9-0377-4dc6-bf80-f5ebad2253e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_dataset = '../data/processed/2100_refined_recsys_papers.feather'\n",
    "authors_dataset = '../data/raw/2000_recsys_authors.feather'\n",
    "\n",
    "clean_papers_dataset = '../data/processed/2200_recsys_papers_cleaned.feather'\n",
    "clean_authors_dataset = '../data/processed/2200_recsys_authors_cleaned.feather'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dff44-ec8a-4647-a2cf-694d90ac15d9",
   "metadata": {},
   "source": [
    "## Load the main papers/authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194b4be-89e4-4133-8f83-6e608f2d39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df = pd.read_feather(papers_dataset)\n",
    "papers_df.shape, papers_df['is_recsys_paper'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b9a00-2ea0-4b6f-9d92-bda1140253e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = pd.read_feather(authors_dataset)\n",
    "authors_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8092edd-cdd3-4d41-99e5-e6ace80cf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates check.\n",
    "papers_df = papers_df.drop_duplicates(subset=['paperId'])\n",
    "papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c73f3-8273-400b-9d98-a3d296a40092",
   "metadata": {},
   "source": [
    "# Check for (and repair) missing author papers\n",
    "One of the issues I have noticed is that the authors dataframe does not have a complete account of publications. There are some papers that can be found in the papers dataframe with a given author that are missing from that authors paper list. We can repair this (at least in part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed11fc-1a59-4181-b167-36929652e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_and_authors_df = (\n",
    "    papers_df\n",
    "    .set_index('paperId')['authors']\n",
    "    .explode().dropna().reset_index()\n",
    "    .rename(columns={'authors': 'authorId'})\n",
    "    .drop_duplicates()\n",
    "    .groupby('authorId')['paperId']\n",
    "    .apply(lambda g: np.unique(g.values))\n",
    ")\n",
    "\n",
    "\n",
    "authors_df = authors_df.set_index('authorId')\n",
    "authors_df['alt_papers'] = papers_and_authors_df\n",
    "authors_df['alt_papers'] = authors_df['alt_papers'].map(\n",
    "    lambda papers: papers if type(papers) is np.ndarray else []\n",
    ")\n",
    "\n",
    "authors_df = authors_df.reset_index('authorId')\n",
    "\n",
    "authors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ad96a-5351-40fc-8a84-495f2030789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "authors_df['updated_papers'] = authors_df.swifter.apply(\n",
    "    lambda author: np.unique(np.concatenate([author['papers'], author['alt_papers']])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "authors_df['num_papers'] = authors_df['updated_papers'].map(len)\n",
    "\n",
    "authors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aff00a-d987-4a22-85dc-ea99847b5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['paperCount'].sum(), authors_df['papers'].map(len).sum(), authors_df['num_papers'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc41f4-fd40-4b4e-8356-ae6bc6a4b2a5",
   "metadata": {},
   "source": [
    "# Check (and repair) the author lists of papers\n",
    "A slghtly different issue is apparent for papers in that some papers list duplicate authors. We folow the same approach as above to resolves this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68442501-7446-47e0-b711-0065ed73d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_and_papers_df = (\n",
    "    authors_df\n",
    "    .set_index('authorId')['papers']\n",
    "    .explode().dropna().reset_index()\n",
    "    .rename(columns={'papers': 'paperId'})\n",
    "    .drop_duplicates()\n",
    "    .groupby('paperId')['authorId']\n",
    "    .apply(lambda g: np.unique(g.values))\n",
    ")\n",
    "\n",
    "papers_df = papers_df.set_index('paperId')\n",
    "papers_df['alt_authors'] = authors_and_papers_df\n",
    "papers_df['alt_authors'] = papers_df['alt_authors'].swifter.apply(\n",
    "    lambda authors: authors if type(authors) is np.ndarray else list()\n",
    ")\n",
    "\n",
    "papers_df = papers_df.reset_index('paperId')\n",
    "\n",
    "papers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb508c1e-f7c7-4250-a870-69a64a7f80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df['updated_authors'] = papers_df.swifter.apply(\n",
    "    lambda paper: np.unique(np.concatenate([paper['authors'], paper['alt_authors']])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Fix the author order so that it matches the original order.\n",
    "papers_df['updated_authors'] = papers_df.swifter.apply(\n",
    "    lambda row: [author for author in row['authors'] if author in row['updated_authors']], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "papers_df['num_authors'] = papers_df['updated_authors'].map(len)\n",
    "\n",
    "papers_df.filter(like='authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4793824a-837c-4cc2-a324-8cf65f6cad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some duplicare authors in the original lists so we are effectiveoy removing these, hence why the author count drops.\n",
    "papers_df['authorCount'].sum(), papers_df['authors'].map(len).sum(), papers_df['num_authors'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509e4f7-848a-4141-834d-d15b0aca9803",
   "metadata": {},
   "source": [
    "# Add author names to papers for convenience\n",
    "Normally we will be dealing with author ids but it is useful to have the corresponding names from the authors dataset in th emain papers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4b96b-0717-429b-8729-c91443bea125",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = authors_df.set_index('authorId')\n",
    "all_author_ids = set(authors_df.index)\n",
    "\n",
    "papers_df['author_names'] = papers_df['updated_authors'].swifter.apply(\n",
    "    lambda author_ids: [\n",
    "        authors_df.loc[author_id]['name'] \n",
    "        for author_id in author_ids \n",
    "        if author_id in all_author_ids\n",
    "    ]\n",
    ")\n",
    "\n",
    "authors_df = authors_df.reset_index()\n",
    "\n",
    "papers_df.filter(like='authors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00513fbd-4860-40b2-a827-0c4807cba8b8",
   "metadata": {},
   "source": [
    "# Clean Venues\n",
    "This is an attempt to clean up some of the messiness that is the venues column. We produce a `clean_venue` column which includes a simplified venue text which facilitates additional matches. It doesn't have a massive effect but it is straightward to do and means that >90% of our papers can be associated with a venue that has >15 papers. This means that <10% of papers are associated with a venue with fewer papers. This could be due to inconsistencies with the way venues are coded or they could just be very small or once-off venues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac473b39-6e94-430f-9a17-37c744d12816",
   "metadata": {},
   "source": [
    "## Simplify venue text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0892d0-a166-4590-bea8-ebbf1f345271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "\n",
    "    # Add the single quote and drop the hyphen\n",
    "    punctuation = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~' + \"â€™\"\n",
    "\n",
    "    # Create a translation table mapping punctuation characters to None\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    \n",
    "    # Remove punctuation using translate method\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_four_digit_years(text):\n",
    "   \n",
    "    # Regular expression pattern to match four-digit years\n",
    "    pattern = r'\\b\\d{4}\\b'\n",
    "    \n",
    "    # Replace all occurrences of the pattern with an empty string\n",
    "    \n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # Remove any extra spaces that may have been left\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def remove_ordinals(text):\n",
    "    \"\"\"\n",
    "    Removes all ordinal numbers from the given string.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "    str: The string with all ordinal numbers removed.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to match ordinal numbers\n",
    "    pattern = r'\\b\\d+(?:st|nd|rd|th)\\b'\n",
    "    # Replace all occurrences of the pattern with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    # Remove any extra spaces that may have been left\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "STOPWORDS = list(STOPWORDS) + ['proceedings']\n",
    "\n",
    "def remove_stopwords(text, stopwords=STOPWORDS):\n",
    "    return ' '.join([\n",
    "        word \n",
    "        for word in text.split() \n",
    "        if (word not in STOPWORDS) & (not(word.isdigit())) & (len(word)>2)\n",
    "    ])\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "def clean_venue(venue):\n",
    "\n",
    "    if venue == '': return np.nan\n",
    "\n",
    "    venue = venue.lower()\n",
    "\n",
    "    venue = remove_punctuation(venue)\n",
    "\n",
    "    venue = remove_ordinals(venue)\n",
    "\n",
    "    venue = remove_four_digit_years(venue)\n",
    "\n",
    "    venue = remove_stopwords(venue)\n",
    "\n",
    "    venue = lemmatize(venue)\n",
    "\n",
    "    venue = venue.replace('international ', '')\n",
    "\n",
    "    return venue\n",
    "\n",
    "\n",
    "papers_df['clean_venue'] = papers_df['venue'].swifter.apply(clean_venue)\n",
    "papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b1751-1eab-44c6-a86b-45b811240c7b",
   "metadata": {},
   "source": [
    "## Swap venue titles for abbreviations for popular venues\n",
    "This is useful later when we want to produce graphs with venues in the axis. Its avoids very long text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08777f-a51c-46bf-89e5-7bb44771e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_venue(df, from_venue, to_venue):\n",
    "\n",
    "    return np.where(\n",
    "        df['clean_venue']==from_venue, \n",
    "        to_venue, \n",
    "        df['clean_venue']\n",
    "    )\n",
    "\n",
    "venue_swaps = [\n",
    "\n",
    "    ('annual acm sigir conference research development information retrieval', 'sigir'),\n",
    "    ('conference information knowledge management', 'cikm'),\n",
    "    ('user modeling adaptation personalization', 'umap'),\n",
    "    ('knowledge discovery data mining', 'kdd'),\n",
    "    ('web search data mining', 'wsdm'),\n",
    "    ('aaai conference artificial intelligence', 'aaai'),\n",
    "    ('joint conference artificial intelligence', 'ijcai'),\n",
    "    ('user modeling user-adapted interaction', 'umuai'),\n",
    "    ('ieee transaction knowledge data engineering', 'tkde'),\n",
    "    ('conference intelligent user interface', 'iui'),\n",
    "    ('european conference information retrieval', 'ecir'),\n",
    "    ('industrial conference data mining', 'icdm'),\n",
    "    ('florida research society', 'flairs'),\n",
    "    ('conference web information system technology', 'wist'),\n",
    "    ('ieee joint conference neural network', 'ijcnn'),\n",
    "    ('applied intelligence boston', 'applied intelligence'),\n",
    "    ('ieee conference big data big data', 'ieee big data'),\n",
    "    ('ieee conference big data', 'ieee big data'),\n",
    "    ('web conference', 'web'),\n",
    "    ('journal research applied science engineering technology', 'ijraset'),\n",
    "    ('conference electronic commerce web technology', 'ecweb'),\n",
    "    ('acm symposium applied computing', 'sac'),\n",
    "    ('expert system application', 'expert systems with applications'),\n",
    "    ('knowledge-based system', 'kbs'),\n",
    "    ('italian information retrieval workshop', 'iir'),\n",
    "    ('journal physic conference series', 'jpcs'),\n",
    "    ('multimedia tool application', 'multimedia tools and applications'),\n",
    "\n",
    "    ('neural information processing system', 'nips'),\n",
    "    ('conference machine learning', 'icml'),\n",
    "    ('annual meeting association computational linguistics', 'acl'),\n",
    "    ('plo one', 'plos one'),\n",
    "    ('conference human factor computing system', 'sigchi'),\n",
    "    ('conference empirical method natural language processing', 'emnlp'),\n",
    "    ('ieee conference acoustic speech signal processing', 'icassp'),\n",
    "    ('social science research network', 'ssrn'),\n",
    "    ('computer vision pattern recognition', 'cvpr'),\n",
    "    ('concurrency computation', 'ccpe'),\n",
    "    ('ieee conference system man cybernetics', 'scm'),\n",
    "    ('global communication conference', 'globecom'),\n",
    "    ('neural computing application print', 'neural computing and applications'),\n",
    "    ('ieee transaction vehicular technology', 'trans vehicular technology'),\n",
    "    ('ieee internet thing journal', 'iotj'),\n",
    "    ('italian national conference sensor', 'sensors'),\n",
    "    ('conference learning representation', 'iclr'),\n",
    "    ('ieee conference data engineering', 'icde'),\n",
    "    \n",
    "    \n",
    "    \n",
    "    ('chi extended abstract', 'sigchi'),\n",
    "    ('chi conference companion', 'sigchi'),\n",
    "    ('sigchi conference human factor computing system', 'sigchi'),\n",
    "    ('extended abstract chi conference human factor computing system', 'sigchi'),\n",
    "    ('chi extended abstract human factor computing system', 'sigchi'),\n",
    "    ('chi conference human factor computing system', 'sigchi'),\n",
    "    \n",
    "    ('communication acm', 'cacm'),\n",
    "    ('acm cacm', 'cacm'),\n",
    "    \n",
    "    ('acm trans interact intell syst', 'transaction interactive intelligent system'),\n",
    "    \n",
    "    ('acm trans inf syst', 'toism'),\n",
    "    ('transaction information system', 'tois'),\n",
    "    \n",
    "    ('acm trans internet techn', 'toit'),\n",
    "    ('transaction internet technology', 'toit'),\n",
    "    \n",
    "    ('acm trans multim comput commun appl', 'tomm'),\n",
    "    \n",
    "    ('acm transaction web', 'tweb'),\n",
    "    \n",
    "    ('web information system engineering', 'wise'),\n",
    "\n",
    "    ('siam data mining', 'sdm'),\n",
    "\n",
    "    ('acm transaction knowledge discovery data', 'tkdd'),\n",
    "    ('transaction knowledge discovery data', 'tkdd'),\n",
    "\n",
    "    ('acm transaction recommender system', 'tors'),\n",
    "    ('transaction recommender system', 'tors'),\n",
    "\n",
    "    ('acm conference recommender system', 'acm recsys'),\n",
    "    ('conference recommender system', 'acm recsys'),\n",
    "    \n",
    "]\n",
    "\n",
    "for venue_swap in venue_swaps:\n",
    "    papers_df['clean_venue'] = fix_venue(papers_df, *venue_swap)\n",
    "\n",
    "papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b27184-2458-4a9f-9662-00174d052051",
   "metadata": {},
   "source": [
    "# Combine/Update Citations\n",
    "Replace the citations of papers with missing citations with the recently scraped citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1be03-5099-4d47-9eb6-ab4aec9dac0c",
   "metadata": {},
   "source": [
    "## Produce an `updated_citations` column\n",
    "This column stores the best estimate of the citations we have for a paper. Either its the original set of citations that were produced in the original crawl of papers or it is based on the citations that were later separately scraped because the original citations were missing or grossly incomplete.\n",
    "\n",
    "Note, we cannot treat the original citation count as true as the updated citations often exceed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63fb4b-802c-476a-b109-1b2f1f2be825",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_orig_citations = papers_df['citations'].notnull()\n",
    "with_scraped_citations = papers_df['scraped_citations'].notnull()\n",
    "\n",
    "\n",
    "# If we have original and scraped citations then get the union.\n",
    "papers_df.loc[with_orig_citations & with_scraped_citations, 'updated_citations'] = (\n",
    "    papers_df[with_orig_citations & with_scraped_citations]\n",
    "    .swifter.apply(lambda row: list(np.union1d(row['citations'], row['scraped_citations'])), axis=1)\n",
    ")\n",
    "\n",
    "papers_df.loc[with_orig_citations & ~with_scraped_citations, 'updated_citations'] = papers_df[with_orig_citations & ~with_scraped_citations]['citations']\n",
    "papers_df.loc[~with_orig_citations & with_scraped_citations, 'updated_citations'] = papers_df[~with_orig_citations & with_scraped_citations]['scraped_citations']\n",
    "\n",
    "papers_df['orig_citation_count'] = papers_df['citations'].map(len)\n",
    "papers_df['updated_citation_count'] = papers_df['updated_citations'].map(len)\n",
    "\n",
    "papers_df[['citationCount', 'orig_citation_count', 'updated_citation_count']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52604f7e-455d-414b-a999-fcf1965e7af6",
   "metadata": {},
   "source": [
    "## Get the years associated with each citation\n",
    "That is, get the year of the citing paper and add as a new column. Each value in this new column will be a year and the sequence of years will correspond to the sequence of citations in the main citations column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf369df-c89c-4c88-b1c3-97648971f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df_by_paper_id = papers_df.set_index('paperId')\n",
    "\n",
    "def is_number(string):\n",
    "    \"\"\"\n",
    "    Checks if the given string is an integer or a real number.\n",
    "\n",
    "    Args:\n",
    "    string (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the string is an integer or a real number, False otherwise.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to match an integer or a real number\n",
    "    pattern = r'^\\d+(\\.\\d+)?$'\n",
    "    # Use fullmatch to ensure the entire string matches the pattern\n",
    "    return bool(re.fullmatch(pattern, string))\n",
    "\n",
    "\n",
    "def get_citation_years(paper_ids):\n",
    "\n",
    "    if len(paper_ids) == 0: \n",
    "        return []\n",
    "\n",
    "    years = papers_df_by_paper_id.reindex(paper_ids)['year'].values\n",
    "\n",
    "    # Remove any missing years; there will be a few but usually a very small fraction only.\n",
    "    return years[~np.isnan(years)]\n",
    "\n",
    "\n",
    "papers_df['citation_years'] = papers_df['updated_citations'].swifter.apply(get_citation_years)\n",
    "\n",
    "papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6e51c-70c2-4a6d-be9c-d461fcd08cd4",
   "metadata": {},
   "source": [
    "# Add Publication Years to the Authors DF\n",
    "Similar to above for citations but it is the publication year of the author publications in the authors dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd9bdc-f0af-4741-9600-90396de5bc21",
   "metadata": {},
   "source": [
    "## Add publication years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e0c73-5280-4ac6-81a7-e6be1ed46dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['publication_years'] = authors_df['updated_papers'].swifter.apply(get_citation_years)\n",
    "authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca618cfe-2775-45f1-bb79-1d933ebe06f2",
   "metadata": {},
   "source": [
    "## Mark the RecSys Papers for authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18545fb-dc83-474b-a539-797fdd3e74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_paper_ids = set(papers_df[papers_df['is_recsys_paper']]['paperId'].unique())\n",
    "len(recsys_paper_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b71be-b54f-403e-9b77-d20c73f221c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['recsys_publications'] = authors_df['updated_papers'].swifter.apply(\n",
    "    lambda papers: [paper for paper in papers if paper in recsys_paper_ids]\n",
    ")\n",
    "\n",
    "authors_df['num_recsys_publications'] = authors_df['recsys_publications'].map(len)\n",
    "\n",
    "authors_df['recsys_publication_years'] = authors_df['recsys_publications'].swifter.apply(get_citation_years)\n",
    "\n",
    "authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e378b3-5050-44c1-990f-7985dfa98699",
   "metadata": {},
   "source": [
    "# Mark the English Papers\n",
    "Not all papers are in English. Here is a simple approach to estimating whether a paper is written in English based on its title/abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85e591-c56d-4e79-9046-a325f2fa3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "\n",
    "    # Add the single quote and drop the hyphen\n",
    "    punctuation = string.punctuation\n",
    "\n",
    "    # Create a translation table mapping punctuation characters to None\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    \n",
    "    # Remove punctuation using translate method\n",
    "    return text.translate(translator)\n",
    "    \n",
    "\n",
    "def get_english_words(text, d=enchant.Dict(\"en\")):\n",
    "\n",
    "    ok_words = set(['recsys', 'recommender', 'movielens', 'grouplens', 'dnn', 'cnn', 'ann', 'cbr', 'ml', 'ai'])\n",
    "\n",
    "    clean_words = remove_punctuation(text.replace('-', ' ')).lower().split()\n",
    "\n",
    "    return [word for word in clean_words if (word in ok_words) | d.check(word)], clean_words\n",
    "\n",
    "def frac_english_words(text):\n",
    "    english_words, clean_words = get_english_words(text)\n",
    "\n",
    "    if len(clean_words):\n",
    "        return len(english_words)/len(clean_words)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "min_frac_english_words = 0.66\n",
    "\n",
    "has_english_title = (\n",
    "    papers_df['title']\n",
    "    .map(lambda text: text.lower())\n",
    "    .swifter.apply(frac_english_words)>=min_frac_english_words\n",
    ")\n",
    "\n",
    "has_english_abstract = (\n",
    "    papers_df['text']\n",
    "    .map(lambda text: text.lower())\n",
    "    .swifter.apply(frac_english_words)>=min_frac_english_words\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "papers_df['has_english_title'] = has_english_title\n",
    "papers_df['has_english_abstract'] = has_english_abstract\n",
    "\n",
    "has_english_title.mean(), has_english_abstract.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e14a5-c11f-46f6-801f-daab558938a1",
   "metadata": {},
   "source": [
    "# Generate Understandable Tokens\n",
    "More tokenization experiments. We tokenize and stem the titles and abstract without removing stopwords. We can do this later if needed but, for example, BERT doesn't want that. Also we reverse/invert the stemmed tokens to restore plausible word (based on mapping frequency) so that we can generate wordclouds and topics out of full words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411cc945-b04f-43f5-a032-9541c18c4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(language='english', ignore_stopwords=True)\n",
    "stemmer = nltk.LancasterStemmer()\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def extract_tokens(text):\n",
    "    \n",
    "    word_list = remove_punctuation(text.replace('-', ' ').lower()).split()\n",
    "    \n",
    "    tokens = [create_token(word) for word in word_list]\n",
    "    \n",
    "    return [token for token in tokens]\n",
    "\n",
    "\n",
    "token_map = {}\n",
    "\n",
    "def create_token(word):\n",
    "    \n",
    "    token = stemmer.stem(lemmatizer.lemmatize(word))\n",
    "\n",
    "    # If the token exists then check the words producing it.\n",
    "    if token in token_map:\n",
    "        word_counts = token_map[token]\n",
    "\n",
    "        # If the word already exists then update its count.\n",
    "        if word in word_counts:\n",
    "            word_counts[word] = word_counts[word]+1\n",
    "            \n",
    "        # Otherwise add a new count.\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "        # Update the token map.\n",
    "        token_map[token] = word_counts\n",
    "\n",
    "    # If there is no token then add a new one with a new word count.\n",
    "    else:\n",
    "        token_map[token] = {word: 1}\n",
    "\n",
    "    return token\n",
    "\n",
    "def reverse_tokens(tokens):\n",
    "\n",
    "    reversed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        word_counts = token_map[token]\n",
    "        reversed_tokens.append(sorted(word_counts.keys(), key=lambda key: word_counts[key], reverse=True)[0])\n",
    "\n",
    "    return reversed_tokens\n",
    "        \n",
    "extract_tokens('Narrative Editing of Web Contexts on Online Community System with Avatar-like Agents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f6e44-faf3-4060-8adb-25293fd6b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df['title_tokens'] = papers_df['title'].swifter.apply(extract_tokens)\n",
    "papers_df['title_tokens_as_string'] = papers_df['title_tokens'].swifter.apply(lambda tokens: ', '.join(tokens))\n",
    "\n",
    "papers_df['reversed_title_tokens'] = papers_df['title_tokens'].swifter.apply(reverse_tokens)\n",
    "papers_df['reversed_title_tokens_as_string'] = papers_df['reversed_title_tokens'].swifter.apply(lambda tokens: ', '.join(tokens))\n",
    "\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1ed57-2d1e-47fd-b037-2767301cdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df['text_tokens'] = papers_df['text'].swifter.apply(extract_tokens)\n",
    "papers_df['text_tokens_as_string'] = papers_df['text_tokens'].swifter.apply(lambda tokens: ', '.join(tokens))\n",
    "\n",
    "papers_df['reversed_text_tokens'] = papers_df['text_tokens'].swifter.apply(reverse_tokens)\n",
    "papers_df['reversed_text_tokens_as_string'] = papers_df['reversed_text_tokens'].swifter.apply(lambda tokens: ', '.join(tokens))\n",
    "\n",
    "papers_df[['title', 'title_tokens_as_string', 'reversed_title_tokens_as_string']].head(20).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354c617-455b-4453-88c0-7535bf41fa3b",
   "metadata": {},
   "source": [
    "# Save the Clean Datasets\n",
    "The final cleaned datasets. In the end we have 58,800 core RS papers within th elarger dataset of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8e0f2-52ad-4143-bc25-184361e3d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df.to_feather(clean_papers_dataset)\n",
    "authors_df.to_feather(clean_authors_dataset)\n",
    "\n",
    "(\n",
    "    papers_df.shape, clean_papers_dataset, \n",
    "    authors_df.shape, clean_authors_dataset,\n",
    "\n",
    "    papers_df['is_recsys_paper'].sum(), papers_df['is_core_recsys_paper'].sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01c097-141a-4f97-a45b-0b878f5ca539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa762e-17e2-44cd-b705-6c4313127bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
