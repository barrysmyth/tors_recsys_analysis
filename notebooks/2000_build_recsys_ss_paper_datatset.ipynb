{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff989e5-6600-417a-b7cc-9824a4332e30",
   "metadata": {},
   "source": [
    "# Build the RecSys SS Dataset\n",
    "The objective of this notebook is to build the main dataset for th euniverse of RS papers. The starting point is a set of seed paper ids in 1000_recsys_paper_ids_52550.feather, which contains a candidate set of paper ids that are considered to be RS papers.\n",
    "\n",
    "These papers are used to generate a list of author papers (papers published by the authors of these seed RS papers) and a list of linked papers, paapers that cite, or are cited by the RS papers.\n",
    "\n",
    "Each of these ids is used to generate the seed papers dataframe -- a dataframe of SS paper records -- and from this we generate a set of author ids and use these to create an author dataframe.\n",
    "\n",
    "All of this data is collected using various API calls to collect papers, authors and citations and requires several hours to run. Care is taken to ensure that paper records are contain a minimal amount of information -- some SS records appear to be little more than stubs with very limited data -- and we also make efforts to deal with some missing information, such as missing citations, by recalling the API. All of this is to say that, this data collection process will be imperfect but sufficient for the purpose of the the analysis we wish to carry out.\n",
    "\n",
    "_**Note from future self:** With the benefit of hindsighht I would strongly recommend performing this type of large scale data collected task using the Semantic Scholar Datasets API because it provides for a more direct route to collecting the data using streamed json files, rather than relying on http requests. That said, the code that is provdied here was used to collect the original data, but be warned it will take >24 hours to run because of API rate limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169050f-d22d-47bc-943e-4272c6775af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import Stemmer\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import string \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import requests\n",
    "from itertools import chain\n",
    "from more_itertools import sliced\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import plt\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob, iglob\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../src/')\n",
    "from semantic_scholar_wrapper import SS\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd682f-2482-4d5f-8365-1a96686fdf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SS(max_attempts=6)\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213812f-6933-411a-99e6-3b7a3189c777",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Various dataset filenames that will use used/produced by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77397c1-96c1-4fd9-a135-d2d1439d90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original set of recsys paper ids used as the starting point for the analysis\n",
    "# The notebook 1000_build_recsys_paper_ids_dataset will recreate this list but it may not produce the same \n",
    "# set of ids at runtime.\n",
    "recsys_paper_ids_dataset = '../data/raw/1000_recsys_paper_ids_52550.feather'\n",
    "\n",
    "# These datasets are produced by this notebook\n",
    "recsys_seed_papers_dataset = '../data/raw/2000_recsys_seed_papers.feather'\n",
    "recsys_seed_authors_dataset = '../data/raw/2000_recsys_seed_authors.feather'\n",
    "\n",
    "recsys_linked_papers_dataset = '../data/raw/2000_recsys_linked_papers.feather'\n",
    "\n",
    "recsys_papers_dataset = '../data/raw/2000_recsys_papers.feather'\n",
    "recsys_authors_dataset = '../data/raw/2000_recsys_authors.feather'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23575f1-27b0-4194-8ff6-4e49df107988",
   "metadata": {},
   "source": [
    "# Load Paper Ids\n",
    "These are the seed RS paper ids identified in an earlier notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f4813-650c-4d04-966b-64d835232a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_seed_ids_df = pd.read_feather(recsys_paper_ids_dataset)\n",
    "recsys_seed_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be0efb-29c2-48fc-9787-2e670d929301",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_paper_ids = list(recsys_seed_ids_df['paperId'])\n",
    "len(seed_paper_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5932f-33a6-4ce1-9a28-e73566ad4406",
   "metadata": {},
   "source": [
    "# Get SS Seed Papers\n",
    "We have the paper ids, now collect the paper records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253d5f1-2e85-406f-a11e-492d3fbef41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_fields = [\n",
    "        'paperId', 'title', 'url', 'venue', 'year', 'journal', 'isOpenAccess',\n",
    "        'publicationTypes', 'publicationDate',\n",
    "        'referenceCount', 'citationCount', 'influentialCitationCount', \n",
    "        'fieldsOfStudy',\n",
    "        'abstract',    \n",
    "        'authors.authorId', 'citations.paperId',  'references.paperId',\n",
    "        'externalIds',\n",
    "        'citationStyles',\n",
    "    ]\n",
    "\n",
    "author_fields = [\n",
    "    'authorId' ,'externalIds' ,'name' ,'affiliations'\n",
    "    ,'paperCount' ,'citationCount' ,'hIndex' ,'papers.paperId'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e81d8-729a-4144-b850-ef2fe806aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_seed_papers = ss.get_papers_in_batches(\n",
    "    seed_paper_ids,\n",
    "    fields=paper_fields,\n",
    "    pool_size=10\n",
    ")\n",
    "\n",
    "recsys_seed_papers_df = ss.items_to_dataframe(recsys_seed_papers)\n",
    "recsys_seed_papers_df.to_feather(recsys_seed_papers_dataset.format(len(recsys_seed_papers_df)))\n",
    "\n",
    "recsys_seed_papers_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46405ad7-0a7f-479c-b723-f307a2eca50b",
   "metadata": {},
   "source": [
    "## Validate Seed Papers\n",
    "To be valid a paper must be sufficiently complete, which means it has a title, year, authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55f06a-b5b9-4c0f-bea6-6c4f0562939d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_papers(papers_df, min_title_length = len('preface.'), valid_years = (1950, 2024)):\n",
    "\n",
    "    has_valid_title = (papers_df['title'].notnull()) & (papers_df['title'].map(len)>min_title_length)\n",
    "    has_valid_year = papers_df['year'].between(*valid_years)\n",
    "    has_valid_authors = papers_df['authors'].map(len)>0\n",
    "\n",
    "    is_valid_seed_paper = has_valid_title & has_valid_year & has_valid_authors\n",
    "\n",
    "    return papers_df[is_valid_seed_paper].copy()\n",
    "\n",
    "recsys_seed_papers_df = validate_papers(recsys_seed_papers_df)\n",
    "recsys_seed_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a9471-a7d9-4ee7-921f-a5db062b3b7f",
   "metadata": {},
   "source": [
    "## Get Seed Authors\n",
    "For each of these valid seed papers we next use the SS API to get information of their authors. This will be used to generate lists of all paper published by RS authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0a343-2ccc-4d80-99bf-ce89a2f4060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_seed_author_ids = list(recsys_seed_papers_df['authors'].explode().dropna().unique())\n",
    "len(recsys_seed_author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e01c6-6478-45ab-ad45-71a02e52e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_seed_authors = ss.get_authors_in_batches(\n",
    "    recsys_seed_author_ids,\n",
    "    fields=author_fields,\n",
    "    pool_size=4\n",
    ")\n",
    "\n",
    "recsys_seed_authors_df = ss.items_to_dataframe(recsys_seed_authors)\n",
    "recsys_seed_authors_df.to_feather(recsys_seed_authors_dataset.format(len(recsys_seed_authors_df)))\n",
    "recsys_seed_authors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7220598-83bf-483f-9bda-67b0e0b73c1c",
   "metadata": {},
   "source": [
    "# Get Linked Papers\n",
    "Similarly, for each of the seed papers we can generate lists of papers that are cited by or that cite these seed papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52efc0f-989c-4bfd-a9a2-5d009b4f91f7",
   "metadata": {},
   "source": [
    "## Seed Cites/Refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c35ebf6-ffa0-4169-b913-b19f5896e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_seed_citation_ids = list(recsys_seed_papers_df['citations'].explode().dropna().unique())\n",
    "len(recsys_seed_citation_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba0f9d9-0d6e-4665-9d68-7e0519cf29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_seed_reference_ids = list(recsys_seed_papers_df['references'].explode().dropna().unique())\n",
    "len(recsys_seed_reference_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daebf95-a1b3-471a-b359-f67d6dcb1615",
   "metadata": {},
   "source": [
    "## Get Seed Author Publications\n",
    "The seed authors publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e71126-5d3f-4688-bbfc-704884d377d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_seed_author_pub_ids = list(recsys_seed_authors_df['papers'].explode().dropna().unique())\n",
    "len(recsys_seed_author_pub_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061dad4-475d-4821-889d-7bf152b2e4c4",
   "metadata": {},
   "source": [
    "## Get the linked papers in batches\n",
    "We have the paper ids for the author pubs, cits and refs, now we need to get the paper records for these cited/citing papers. It's a long process so we do this in batches so that if it terminates prematurely it is easier to restart from where we left off. Not ideal but good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0274bb63-f45f-4841-8ff9-ba50122d4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_in_parts(\n",
    "    ids, part_filename_template, \n",
    "    get_items_fn=ss.get_papers_in_batches, item_fields=paper_fields, \n",
    "    batch_size=500, pool_size=10, part_size=500_000, restart_at_part=0\n",
    "):\n",
    "\n",
    "    ids_in_parts = list(sliced(ids, part_size))\n",
    "\n",
    "    part_filenames = []\n",
    "    \n",
    "    for i, part in enumerate(ids_in_parts[restart_at_part:]):\n",
    "    \n",
    "        logger.info((i+restart_at_part, len(part), part_filename_template.format(i+restart_at_part)))\n",
    "    \n",
    "        # Get the next group of papers\n",
    "        items = get_items_fn(\n",
    "            part, fields=item_fields, \n",
    "            batch_size=batch_size, pool_size=pool_size\n",
    "        )\n",
    "    \n",
    "        # Convert to a df\n",
    "        items_df = ss.items_to_dataframe(items, pool_size=24)\n",
    "    \n",
    "        # Save the df\n",
    "        part_filename = part_filename_template.format(i+restart_at_part)\n",
    "        items_df.to_feather(part_filename)\n",
    "        part_filenames.append(part_filename)\n",
    "    \n",
    "        # Free up the memory\n",
    "        del items_df\n",
    "\n",
    "    return part_filenames\n",
    "    \n",
    "\n",
    "def assemble_parts(part_filenames, pool_size=10):\n",
    "\n",
    "    with Pool(pool_size) as p:\n",
    "\n",
    "        dfs = p.map(pd.read_feather, part_filenames)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac428b-8b58-4a0c-815d-31720734cf30",
   "metadata": {},
   "source": [
    "Combine the cites, refs and author pubs to produce a large list of papers ids for which we need paper records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82209dd1-796e-4050-8260-cda4ce04eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_paper_ids = list(set(recsys_seed_citation_ids).union(set(recsys_seed_reference_ids)).union(recsys_seed_author_pub_ids))\n",
    "len(linked_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20acc170-e71e-4412-914d-55ba2dc22ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_filenames = crawl_in_parts(\n",
    "    linked_paper_ids, \n",
    "    '../data/raw/parts/2000_linked_papers_part_{}_{}.feather',\n",
    "    pool_size=15\n",
    ")\n",
    "part_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420c7ae-43ab-4ee5-9767-81d129f54897",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_linked_papers_df = assemble_parts(part_filenames)\n",
    "recsys_linked_papers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c21a97-74ca-4268-94b8-2d1236de2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_linked_papers_df = validate_papers(recsys_linked_papers_df)\n",
    "recsys_linked_papers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef419e67-fd81-4507-8426-772dbe4dc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_linked_papers_df.to_feather(recsys_linked_papers_dataset)\n",
    "recsys_linked_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe20887-d634-431b-807b-8f3c9b97d66f",
   "metadata": {},
   "source": [
    "# Create RecSys Papers Dataset\n",
    "Produce the main RS datarame by combining the seed papers and linked papers (including authors pubs). We add some additional columns which will be useful later, maybe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057c559-ac37-4ca3-889e-e47e1da5c157",
   "metadata": {},
   "source": [
    "## Combine Seed & Linked DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31754a-96ef-403d-9eab-233bdb114162",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_papers_df = (\n",
    "    pd\n",
    "    .concat([recsys_seed_papers_df, recsys_linked_papers_df], ignore_index=True)\n",
    "    .drop_duplicates(subset=['paperId'])\n",
    ")\n",
    "\n",
    "recsys_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9a97a-add0-4d9a-87d1-127468f247fb",
   "metadata": {},
   "source": [
    "## Some Additional Cols\n",
    "Strictly speaking these additional columns should have neen added in a later notebook to decouple this aspect from the data collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfadfad-93bd-4b38-bc95-d98f7d59c9ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fix the Fields of Study so that they always contain lists; this will simplify things later.\n",
    "recsys_papers_df['fieldsOfStudy'] = recsys_papers_df['fieldsOfStudy'].map(\n",
    "    lambda f:  list(f) if (type(f) is np.ndarray) | (type(f) is list) else []\n",
    ")\n",
    "\n",
    "recsys_papers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb2bcb-89d1-4199-8948-090a950a4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_papers_df['authorCount'] = recsys_papers_df['authors'].map(len)\n",
    "recsys_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c8ce8-b9e3-4125-b132-42e9ea77b39d",
   "metadata": {},
   "source": [
    "### Paper source indicator cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678dd5c-383e-4f63-af64-11664adef349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator cols --this helps us understand where the paper came from (seed, citation, ref, author pub).\n",
    "recsys_papers_df['is_seed'] = recsys_papers_df['paperId'].isin(seed_paper_ids)\n",
    "recsys_papers_df['is_seed_citation'] = recsys_papers_df['paperId'].isin(recsys_seed_citation_ids)\n",
    "recsys_papers_df['is_seed_reference'] = recsys_papers_df['paperId'].isin(recsys_seed_reference_ids)\n",
    "recsys_papers_df['is_seed_author_pub'] = recsys_papers_df['paperId'].isin(recsys_seed_author_pub_ids)\n",
    "\n",
    "recsys_papers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7c751-65db-4503-9625-3550f6dd5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some (<40) papers whose ids do not appear in our seed or linked lists.\n",
    "# These are ids that map to some other id by SS. We remove them because they cannot be tracked.\n",
    "recsys_papers_df = recsys_papers_df[recsys_papers_df.filter(like='is_seed').any(axis=1)].copy()\n",
    "recsys_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122f3b0-2155-46a7-a026-63bddf33c152",
   "metadata": {},
   "source": [
    "### Combining title & abstract; removing punctuation\n",
    "Add a `text` field base don a normalised verion of the title and abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa911b-9e1c-488e-9e21-124c531cb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "\n",
    "    # Add the single quote and drop the hyphen\n",
    "    punctuation = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~' + \"â€™\"\n",
    "\n",
    "    # Create a translation table mapping punctuation characters to None\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    \n",
    "    # Remove punctuation using translate method\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "# Combining titles and abstract; remove punctuation.\n",
    "recsys_papers_df['text'] = (\n",
    "    recsys_papers_df['title'].map(lambda s: s if type(s) is str else '') \\\n",
    "    + ' ' +\\\n",
    "    recsys_papers_df['abstract'].map(lambda s: s if type(s) is str else '') \\\n",
    "    + ' ' +\\\n",
    "    recsys_papers_df['venue'].map(lambda s: s if type(s) is str else '')\n",
    "\n",
    ").str.lower().map(remove_punctuation)\n",
    "\n",
    "recsys_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302eae3f-7257-4ce7-836d-f0722b865a2d",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Some stemmed versions of the title/abstract text. May use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0666e6-23c9-4413-8068-2bb53d078348",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "def remove_stop_words(text, stop_words=stop_words):\n",
    "    word_list = text.split()\n",
    "    return ' '.join([word for word in word_list if word not in stop_words])\n",
    "\n",
    "recsys_papers_df['text_without_stop_words'] = recsys_papers_df['text'].swifter.apply(remove_stop_words)\n",
    "\n",
    "\n",
    "def stem_words(text):\n",
    "    word_list = text.split()\n",
    "    return ' '.join(stemmer.stemWords(word_list))\n",
    "\n",
    "recsys_papers_df['stemmed_text'] = recsys_papers_df['text'].swifter.apply(stem_words)\n",
    "\n",
    "recsys_papers_df['stemmed_text_without_stop_words'] = recsys_papers_df['text_without_stop_words'].swifter.apply(stem_words)\n",
    "    \n",
    "\n",
    "recsys_papers_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1cda3-2482-4648-a52e-1059f7a5728e",
   "metadata": {},
   "source": [
    "### Adding nGrams\n",
    "Not sure if we will use these ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82434093-dbfd-4e29-a9b7-04080d19fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split(' ')\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngrams.append('_'.join(words[i:i+n]))\n",
    "    return ngrams\n",
    "        \n",
    "recsys_papers_df['ngrams_without_stop_words'] = (\n",
    "    recsys_papers_df['stemmed_text_without_stop_words']\n",
    "    .swifter.apply(lambda text: list(set(list(chain.from_iterable(\n",
    "        [generate_ngrams(text, n) for n in range(2, 4)])))))\n",
    ")\n",
    "\n",
    "# recsys_papers_df['num_ngrams'] = recsys_papers_df['ngrams'].map(len)\n",
    "recsys_papers_df['num_ngrams_without_stop_words'] = recsys_papers_df['ngrams_without_stop_words'].map(len)\n",
    "\n",
    "recsys_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f43ef3-c053-4685-8922-3cdaaea24806",
   "metadata": {},
   "source": [
    "## Mark the RecSys Papers\n",
    "Now we can mark the candidate recsys papers, which will include the original seed set but may also include papers that we have found in the linked and author pub sets. To do this we use the same query set that we used to identify our original se of seed papets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0b7ff-da3e-40ea-9a68-233fd27591be",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_queries = [\n",
    "    '\"recommender system\"', '\"recommendation system\"', \n",
    "    '\"collaborative filter\"', '\"collaborative recommend\"',\n",
    "    '\"social information filter\"', '\"collaborative information filter\"',\n",
    "    '\"user-item\"',\n",
    "    'recsys', 'grouplens', 'movielens', '\"netflix prize\"',\n",
    "]\n",
    "\n",
    "# To check DBLP titles we dont need the quotes and we will add 'recommender'\n",
    "recsys_phrases = [q.replace('\"', '') for q in recsys_queries] + ['recommender']\n",
    "\n",
    "\n",
    "# Check which papers contain these phrases.\n",
    "def contains_phrases(text, phrases=recsys_phrases):\n",
    "    found_phrases = [phrase for phrase in phrases if phrase in text]\n",
    "\n",
    "    return found_phrases\n",
    "\n",
    "contains_recsys_phrases = recsys_papers_df['text'].swifter.apply(contains_phrases)\n",
    "\n",
    "# The matching phrases for each paper.\n",
    "recsys_papers_df['matching_recsys_phrases'] = contains_recsys_phrases\n",
    "\n",
    "# The number of these matching phrases.\n",
    "recsys_papers_df['num_recsys_phrases'] = contains_recsys_phrases.map(len)\n",
    "\n",
    "# A paper is a recsys paper if it matches at least one recsys phrase; this is likely too weak.\n",
    "recsys_papers_df['is_recsys_paper'] = recsys_papers_df['num_recsys_phrases']>0\n",
    "\n",
    "recsys_papers_df.shape, recsys_papers_df['is_seed'].sum(), recsys_papers_df['is_recsys_paper'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9babe-2deb-42b7-b50a-15e449a8bf15",
   "metadata": {},
   "source": [
    "# Final Set of Authors\n",
    "Since we have added a few more recsys papers we need to update the authors too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64a279-9185-49e2-bc5b-da3ca7406c2f",
   "metadata": {},
   "source": [
    "## Identify & Collect Missing Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed0a0d-93bf-44be-86c5-bc6172db2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recsys_author_ids = set(\n",
    "    recsys_papers_df[recsys_papers_df['is_recsys_paper']]['authors']\n",
    "    .explode().dropna().unique()\n",
    ")\n",
    "\n",
    "missing_recsys_author_ids = list(all_recsys_author_ids.difference(set(recsys_seed_authors_df['authorId'].unique())))\n",
    "len(missing_recsys_author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c59d74-de8c-4409-97bf-ae0a8e8ab596",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_recsys_authors = ss.get_authors_in_batches(\n",
    "    missing_recsys_author_ids,\n",
    "    fields=author_fields,\n",
    "    pool_size=4\n",
    ")\n",
    "\n",
    "missing_recsys_authors_df = ss.items_to_dataframe(missing_recsys_authors)\n",
    "missing_recsys_authors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97304d7-326d-409b-ab4b-d5d5b4ba8de6",
   "metadata": {},
   "source": [
    "## Combine with the seed authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd74f95-f122-4bba-9350-d2d9dbd9012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_authors_df = (\n",
    "    pd\n",
    "    .concat([recsys_seed_authors_df, missing_recsys_authors_df], ignore_index=True)\n",
    "    .drop_duplicates(subset=['authorId'])\n",
    ")\n",
    "\n",
    "recsys_authors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45961043-ffbd-4570-86d4-45805aeb629e",
   "metadata": {},
   "source": [
    "## Sort author publications by year\n",
    "I thkn that the author pubs may already be sorted by year but here we make sure out of an abdundance of caution. A slight hitch is that we cannot guarantee that every publication id is in our dataset because SS does not find everything even when it seems to have a valid id. Fortunately, >80% of pub ids are in the dataset and so we can get their year and sort accordingly. This should be sufficient for whatever we want to do, if anything, with this sorted publication data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc26972-7e5a-49e6-b7e1-20fd641d2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexed version of recsys_papers_df to speedup the lookups\n",
    "recsys_papers_df_indexed_by_id = recsys_papers_df.set_index('paperId')\n",
    "\n",
    "# The papers that are in our papers df; This is just over 80% of the ids which \n",
    "# should be sufficient.\n",
    "recsys_authors_df['available_papers'] = recsys_authors_df['papers'].swifter.apply(\n",
    "    lambda ids: [id for id in ids if id in recsys_papers_df_indexed_by_id.index]\n",
    ")\n",
    "\n",
    "# Sort these paper ids by year.\n",
    "recsys_authors_df['sorted_papers'] = (\n",
    "    recsys_authors_df['available_papers']\n",
    "    .swifter\n",
    "    .apply(\n",
    "        lambda ids: sorted(\n",
    "            ids, \n",
    "            key=lambda id: recsys_papers_df_indexed_by_id.loc[id]['year']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "recsys_authors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11ee7f-c275-49b0-bac9-a7b02db0a430",
   "metadata": {},
   "source": [
    "# Scrape as many missing citations as we can ...\n",
    "Turns out the citation lists are hit and miss. So for now we can do another crawl to get the citations for each paper. We dont want to do this for every paper though. Just the ones with lots of missing cites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dcce8-4dc2-41b4-b74c-b30e4bba8502",
   "metadata": {},
   "source": [
    "## The papers with enough missing citations to recrawl\n",
    "We focus in on papers weher we have have fewer than 95% of their citation count and where this is more than 5 missing cites. This avoids the need to crawl papers with very low citation counts. I tried using `get_citations` for this but it is way too slow because we are limited to 1 paper per second or about 60k per day. The alternative is to use `get_papers` but to only ask for the citation info and limit the batch size to about 100 papers. This seems to get citations for 95% of the papers which is pretty decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc115718-8da6-4ac2-a82d-9ed81eb19fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_found_citations = recsys_papers_df['citations'].map(len)\n",
    "\n",
    "min_frac_citations = 0.95\n",
    "min_missing_citations = 5\n",
    "\n",
    "frac_found_citations = num_found_citations/recsys_papers_df['citationCount']\n",
    "\n",
    "num_missing_citations = recsys_papers_df['citationCount']-num_found_citations\n",
    "\n",
    "with_missing_citations = (frac_found_citations<min_frac_citations) & (num_missing_citations>min_missing_citations)\n",
    "\n",
    "\n",
    "papers_ids_with_missing_citations = list(recsys_papers_df[with_missing_citations]['paperId'].unique())\n",
    "len(papers_ids_with_missing_citations), papers_ids_with_missing_citations[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5412296-4b7b-4a23-97f1-04715c7185b9",
   "metadata": {},
   "source": [
    "## Scrape the citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79462eb-77b9-4908-a4b4-d628a21ab3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_filenames = crawl_in_parts(\n",
    "    papers_ids_with_missing_citations, \n",
    "    '../data/raw/parts/2000_missing_citations_part_{}.feather',\n",
    "    item_fields=['paperId', 'citationCount', 'citations.paperId'],\n",
    "    batch_size=20,      # Trial and error suggest that this gives the best results.\n",
    "    part_size=100_000,\n",
    "    pool_size=4,\n",
    ")\n",
    "part_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b84dd-f0c8-49a6-a6b4-5f663603bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_citations_df = assemble_parts(part_filenames).set_index('paperId').add_prefix('scraped_')\n",
    "missing_citations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec1e6a-14dc-405d-acac-71c3607c9f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T13:41:11.640418Z",
     "iopub.status.busy": "2024-06-24T13:41:11.639635Z",
     "iopub.status.idle": "2024-06-24T13:43:00.603829Z",
     "shell.execute_reply": "2024-06-24T13:43:00.602752Z",
     "shell.execute_reply.started": "2024-06-24T13:41:11.640368Z"
    },
    "scrolled": true
   },
   "source": [
    "## Add the missing citations data to the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc4b67-69fe-4b24-b4c3-b4e527cbe0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_papers_df = recsys_papers_df.set_index('paperId').join(missing_citations_df, how='left').reset_index()\n",
    "recsys_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99f1b4-7f14-4a1f-ae1b-f01c10f048f9",
   "metadata": {},
   "source": [
    "# Save RecSys Datasets\n",
    "Save the dataset of papers and the dataset of authors. These define the broader RS universe and contain RS specific papers/authors. They will be further refined and cleaned and used as the basis of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660f1b5-b74b-45d2-a090-7458834c0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_papers_df.to_feather(recsys_papers_dataset)\n",
    "recsys_papers_df.shape, recsys_papers_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70bc42c-53bd-4f83-937c-ace0c21d1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_authors_df.to_feather(recsys_authors_dataset)\n",
    "recsys_authors_df.shape, recsys_authors_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3bbbdc-8b0a-4587-9875-524c133738ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
