{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14370a17-f43f-4653-82ac-08408424ea15",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "In this notebook we generate BERT topic models for our RS papers and the wider usniverse of papers. This is a time consuming and labour intensive process. I tried various topic modelling settings (e.g. min topic size etc.) before landing on a given setting which produced an acceptable topic model for the universe and RS papers.\n",
    "\n",
    "To save time and effort best models for the universe and RS papers are incldued in:\n",
    "\n",
    "* Universe Model - ../data/models/2400_best_topic_model_df_all_with_recsys_47_5000.pkl (.csv also)\n",
    "* RS Model - ../data/models/2400_best_topic_model_df_recsys_only_42_200.pkl (.csv also)\n",
    "\n",
    "The above were used in the remaining analysis and rather than re-running an expensive topic modelling process to reproduce similar models, th einterested researcher can instead skip this notebook and use the 2410_ notebook to incorporate the above models into th emain papers datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be619b3d-cb3c-4431-830f-88d501ff7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import Stemmer\n",
    "\n",
    "import os\n",
    "# Should prevent \"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. \" warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "import string \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "# nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob, iglob\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance, KeyBERTInspired\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True) \n",
    "\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz\n",
    "\n",
    "#Import NLP librarys and the spacy package to preprocess the abstract text\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS #import commen list of stopword\n",
    "# import en_core_sci_lg  # import downlaoded model\n",
    "import string\n",
    "\n",
    "from minisom import MiniSom  \n",
    "from sklearn.cluster import SpectralClustering \n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6068b6-2be7-44a4-82e9-f7a5568c2928",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72167b9-0377-4dc6-bf80-f5ebad2253e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_dataset = '../data/processed/2300_recsys_universe_papers.feather'\n",
    "# '../data/processed/2200_recsys_papers_cleaned.feather'\n",
    "\n",
    "papers_df = pd.read_feather(papers_dataset)\n",
    "\n",
    "papers_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebbe971-79db-4af5-b340-2066c20d9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_dataset = '../data/processed/2300_inside_outside_papers.feather'\n",
    "\n",
    "recsys_papers_df = pd.read_feather(recsys_dataset)\n",
    "\n",
    "recsys_papers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba9886-211e-4825-b62c-c18d6bf66eb6",
   "metadata": {},
   "source": [
    "# Process for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c90815-b2e8-49c0-b5aa-3fe3f8b4d104",
   "metadata": {},
   "source": [
    "## Clean up the title/abstract texts\n",
    "Some sbtracts seem to just contain coneference names and regions. Note that we are not removing stop words here as per the advice on the BERT project page.\n",
    "\n",
    "* From https://maartengr.github.io/BERTopic/faq.html#how-do-i-reduce-topic-outliers: ``At times, stop words might end up in our topic representations. This is something we typically want to avoid as they contribute little to the interpretation of the topics. However, removing stop words as a preprocessing step is not advised as the transformer-based embedding models that we use need the full context to create accurate embeddings.``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5ea4a-dd53-42b9-9e29-272b8ddcb7a5",
   "metadata": {},
   "source": [
    "## Normalise the titles and text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9589ad1-aa32-4e65-9337-1fe2c831f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df['clean_title'] = papers_df['title'].map(lambda s: s.lower())\n",
    "recsys_papers_df['clean_title'] = recsys_papers_df['title'].map(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b6327-62d6-4f8e-9ff0-cdab6312f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "\n",
    "    # Add the single quote and drop the hyphen\n",
    "    punctuation = string.punctuation\n",
    "\n",
    "    # Create a translation table mapping punctuation characters to None\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    \n",
    "    # Remove punctuation using translate method\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "drop_words = set(['conference', 'international', 'journal', 'acm', 'ieee', 'springer', 'elsevier', 'transactions', 'nationa', 'symposium', 'workshop'])\n",
    "\n",
    "papers_df['clean_text'] = papers_df['text'].swifter.apply(\n",
    "    lambda text: ' '.join([\n",
    "        word\n",
    "        for word in remove_punctuation(text.replace('-', ' ')).lower().split()\n",
    "        if word not in drop_words\n",
    "    ]))\n",
    "\n",
    "recsys_papers_df['clean_text'] = recsys_papers_df['text'].swifter.apply(\n",
    "    lambda text: ' '.join([\n",
    "        word\n",
    "        for word in remove_punctuation(text.replace('-', ' ')).lower().split()\n",
    "        if word not in drop_words\n",
    "    ]))\n",
    "\n",
    "recsys_papers_df['clean_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca19ef8-cf0e-4d9a-913d-3500a275a9de",
   "metadata": {},
   "source": [
    "## Focus on papers with english titles and something in the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207dbdf-e5cc-4b4b-82bc-1c725d22da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_has_english_title = papers_df['has_english_title']\n",
    "paper_has_english_abstract = papers_df['has_english_abstract']\n",
    "papers_with_abstracts = papers_df['clean_text'].map(len)>(papers_df['title'].map(len)+25)\n",
    "\n",
    "use_english_papers = paper_has_english_abstract & paper_has_english_title\n",
    "\n",
    "english_papers_df = papers_df[use_english_papers & papers_with_abstracts].copy()\n",
    "\n",
    "english_papers_df.shape, len(english_papers_df)/len(papers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d7a30-b607-42e9-8402-80f2473cf17b",
   "metadata": {},
   "source": [
    "# Topic Modeling for the RecSys Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b77ee-c5e0-4a00-958e-7eab35d7fa99",
   "metadata": {},
   "source": [
    "## Get the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a067722-4463-4b87-a3a8-ecf04f961b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_papers_with_english_title = recsys_papers_df['has_english_title']\n",
    "recsys_papers_with_english_title.mean(), recsys_papers_with_english_title.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32d727-837f-46fa-bbbc-32402f4f4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_paper_ids = set(recsys_papers_df[recsys_papers_with_english_title]['paperId'].unique())\n",
    "\n",
    "recsys_docs = recsys_papers_df[recsys_papers_with_english_title].set_index('paperId')['clean_text']\n",
    "recsys_titles = recsys_papers_df[recsys_papers_with_english_title].set_index('paperId')['clean_title']\n",
    "\n",
    "len(recsys_paper_ids), recsys_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7231e-fd3b-4f91-ac0a-1784067dc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_papers_df_by_id = english_papers_df.set_index('paperId')\n",
    "english_papers_df_by_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b2cc7-9818-466b-a4e8-8faddad9c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_english_with_recsys_docs = english_papers_df_by_id['clean_text']\n",
    "all_english_with_recsys_titles = english_papers_df_by_id['clean_title']\n",
    "\n",
    "all_english_with_recsys_docs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273897c-ee3e-438f-9c3e-8b47f46de53a",
   "metadata": {},
   "source": [
    "## Build the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6359c-6e73-4197-9583-202129ab85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")    # SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def gen_topic_model(docs, embeddings, min_topic_size, nr_topics, sentence_model=sentence_model, random_state=random_state): \n",
    "\n",
    "    top_n_words = 100\n",
    "    \n",
    "    dim_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=random_state)\n",
    "    \n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    representation_model = MaximalMarginalRelevance(top_n_words=top_n_words, diversity=0.33)\n",
    "\n",
    "    logger.info('Building model...')\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=sentence_model,\n",
    "        umap_model=dim_model, \n",
    "        vectorizer_model=vectorizer_model, \n",
    "        calculate_probabilities=False, \n",
    "        min_topic_size=min_topic_size,\n",
    "        top_n_words=top_n_words,\n",
    "        low_memory=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    topics, _ = topic_model.fit_transform(docs, embeddings)\n",
    "    logger.info('Model has {} topics'.format(len(set(topics))))\n",
    "\n",
    "    logger.info('Reducing outliers...')\n",
    "    new_topics = topic_model.reduce_outliers(docs, topics, strategy=\"embeddings\")  \n",
    "    topic_model.update_topics(docs, topics=new_topics, vectorizer_model=vectorizer_model, representation_model=representation_model, top_n_words=top_n_words)\n",
    "\n",
    "    if nr_topics: topic_model.reduce_topics(docs, nr_topics=nr_topics)\n",
    "    \n",
    "    return new_topics, topic_model\n",
    "\n",
    "\n",
    "\n",
    "def gen_adj_topic_name(word_list, n=4):\n",
    "\n",
    "    drop_tokens = set(['cr', 'cdr', 'iptv', 'mob', 'wkh', 'artic', 'xvhu', 'dqg', 'acm', 'bm3d', 'la', 'bas', 'method'])\n",
    "    \n",
    "    tokenizer = Tokenizer(stemmer=LancasterStemmer())\n",
    "\n",
    "    # Get tokens, remove duplicates, preserve order\n",
    "    tokens = list(dict.fromkeys([token for token in tokenizer.words_to_tokens(word_list) if token not in drop_tokens]))\n",
    "    words = tokenizer.tokens_to_words(tokens)\n",
    "\n",
    "    return ', '.join(words[:n]).title()\n",
    "\n",
    "\n",
    "\n",
    "def build_topics(docs, min_topic_size, nr_topics):\n",
    "\n",
    "    doc_text = docs.to_numpy()\n",
    "    \n",
    "    embeddings = sentence_model.encode(doc_text, show_progress_bar=True)\n",
    "    \n",
    "    topics, topic_model = gen_topic_model(doc_text, embeddings, min_topic_size=min_topic_size, nr_topics=nr_topics)\n",
    "    \n",
    "    topic_model_df = topic_model.get_topic_info()\n",
    "    topic_model_df.columns = ['topic_id', 'topic_count', 'topic_name', 'topic_representation', 'topic_representative_docs']\n",
    "\n",
    "    topic_model_df['top_n_words'] = topic_model_df['topic_id'].map(lambda topic_id: topic_model.get_topic(topic_id))\n",
    "\n",
    "    papers_by_topic = (\n",
    "        pd\n",
    "        .DataFrame({'topic_id': topics, 'paper_id': docs.index})\n",
    "        .groupby('topic_id')\n",
    "        .apply(lambda g: np.concatenate(g.values), include_groups=False)\n",
    "    )\n",
    "\n",
    "    topic_model_df = topic_model_df.set_index('topic_id')\n",
    "    topic_model_df['papers'] = papers_by_topic\n",
    "\n",
    "    topic_model_df['adj_topic_name'] = topic_model_df['topic_representation'].map(gen_adj_topic_name)\n",
    "\n",
    "\n",
    "    return topic_model_df, topic_model, embeddings\n",
    "\n",
    "\n",
    "def build_and_save_model(docs, min_topic_size, nr_topics, label):\n",
    "\n",
    "    # Build the model\n",
    "    topics_df, topic_model, embeddings = build_topics(docs, min_topic_size=min_topic_size, nr_topics=nr_topics)\n",
    "\n",
    "    # Save the model and the model df\n",
    "    topic_model.save(\"../data/models/3400_topic_model_{}_{}_{}.pkl\".format(label, len(topics_df), min_topic_size), serialization=\"pickle\")\n",
    "    topics_df.to_csv(\"../data/models/3400_topic_model_df_{}_{}_{}.csv\".format(label, len(topics_df), min_topic_size), index=True)\n",
    "    topics_df.to_pickle(\"../data/models/3400_topic_model_df_{}_{}_{}.pkl\".format(label, len(topics_df), min_topic_size))\n",
    "\n",
    "    return topics_df, topic_model, embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018f48e-ceeb-4323-88ff-9289bb915f1c",
   "metadata": {},
   "source": [
    "# Build the Universe Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70912a-e29a-4a1c-a08f-fcb4520fac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_with_recsys_topics_df, all_with_recsys_topic_model, all_with_recsys_embeddings = build_and_save_model(\n",
    "    all_english_with_recsys_docs, min_topic_size=5000, nr_topics=False, label='all_with_recsys'\n",
    ")\n",
    "\n",
    "all_with_recsys_topics_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74362d3-ae14-4ebf-b01c-6f90ac6f4420",
   "metadata": {},
   "source": [
    "# Build the RecSys Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9575d7-0f55-4dc9-9ca9-dc71e20839ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_topics_df, recsys_topic_model, recsys_embeddings = build_and_save_model(recsys_docs, min_topic_size=200, label='recsys_only')\n",
    "len(recsys_docs), recsys_topics_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
