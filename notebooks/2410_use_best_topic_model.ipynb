{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc1d04b-9f33-4869-bb15-0e3cb45fde1e",
   "metadata": {},
   "source": [
    "# Combining Topics & Papers\n",
    "The purpose of this notebook is to combine the topic analysis with the main datasets. We have experimented with various different settings to produce several variations of the topic analyses (e.g. based on various minimum topic sizes etc.) Here we use the best performing models. The modules themselves are available in this repo in the models/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be619b3d-cb3c-4431-830f-88d501ff7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import Stemmer\n",
    "\n",
    "import os\n",
    "# Should prevent \"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. \" warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "import string \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "# nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob, iglob\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6068b6-2be7-44a4-82e9-f7a5568c2928",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428ece4-26d1-4849-9a47-de4889363e6c",
   "metadata": {},
   "source": [
    "## The Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3e74a-0f5a-4793-bd5b-53e51c6430e4",
   "metadata": {},
   "source": [
    "### The Universe Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3ef12-80f8-4cc8-8ddb-a4d9c58b8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_dataset = '../data/processed/2300_recsys_universe_papers.feather'\n",
    "all_papers_df = pd.read_feather(all_papers_dataset)\n",
    "all_papers_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f7912-1d5a-46e7-a293-3f92192f8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_by_id = all_papers_df.set_index('paperId')\n",
    "all_papers_by_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa527fc-e4a1-4248-94b3-81b21a8a29cd",
   "metadata": {},
   "source": [
    "### The RecSys Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2b167-839e-47bf-8ddf-bd470cbb85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_papers_df = pd.read_feather('../data/processed/2300_inside_outside_papers.feather')\n",
    "recsys_papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a69472-6654-4859-bdd9-c839b493f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_papers_by_id = recsys_papers_df.set_index('paperId')\n",
    "recsys_papers_by_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5a626-5741-4680-b281-1f7e192c9230",
   "metadata": {},
   "source": [
    "## The Topic Models\n",
    "The topic models below already exist in this repo and do not have to be regenerated. In case there are issues with th e.pkl versions, .csv files have also been included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d07335-f1f5-4a14-9096-ecb6c967f9db",
   "metadata": {},
   "source": [
    "### The Universe Topics (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ba726-c01a-45ab-bfb7-3db0fb84dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_all_topic_model_df_dataset = '../data/models/2400_best_topic_model_df_all_with_recsys_47_5000.pkl'\n",
    "best_all_topic_model_df = pd.read_pickle(best_all_topic_model_df_dataset)\n",
    "best_all_topic_model_df.shape              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadaed4c-7c64-4032-8528-25c97a4dcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any topic papers that are not in the universe.\n",
    "\n",
    "all_topic_model_df = best_all_topic_model_df.copy()\n",
    "\n",
    "all_topic_model_df['papers'] = all_topic_model_df['papers'].swifter.apply(\n",
    "    lambda papers: [paper for paper in papers if paper in all_papers_by_id.index]\n",
    ")\n",
    "\n",
    "all_topic_model_df['topic_count'] = all_topic_model_df['papers'].map(len)\n",
    "\n",
    "all_topic_model_df['topic_count'].sum(), len(all_papers_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001dffe9-401a-40fe-bfc0-5deef410b4de",
   "metadata": {},
   "source": [
    "### The RecSys Topics (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205f1c1-19cb-4e58-a09c-c51019b5ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recsys_topic_model_df_dataset = '../data/models/2400_best_topic_model_df_recsys_only_42_200.pkl'\n",
    "best_recsys_topic_model_df = pd.read_pickle(best_recsys_topic_model_df_dataset)\n",
    "best_recsys_topic_model_df.shape\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35a649-76b7-4e0f-8074-956d60a67032",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recsys_topic_model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a65a08-43a7-4279-b243-f27fd007a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any topic papers that are not in the universe.\n",
    "\n",
    "recsys_topic_model_df = best_recsys_topic_model_df.copy()\n",
    "\n",
    "recsys_topic_model_df['papers'] = recsys_topic_model_df['papers'].swifter.apply(\n",
    "    lambda papers: [paper for paper in papers if paper in recsys_papers_by_id.index]\n",
    ")\n",
    "\n",
    "recsys_topic_model_df['topic_count'] = recsys_topic_model_df['papers'].map(len)\n",
    "\n",
    "recsys_topic_model_df['topic_count'].sum(), len(recsys_papers_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4547f7-3bd7-4e52-8f7a-5ad1ba317c4d",
   "metadata": {},
   "source": [
    "## Improve RecSys Topic Names\n",
    "We drop thigs like 'recommender' from the topic representations as its not especially useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e3fa2-3b18-497c-9102-ab01d9fccf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def  __init__(self, stemmer=PorterStemmer(), lemmatizer=WordNetLemmatizer()\n",
    "):\n",
    "\n",
    "        self.token_map = {}\n",
    "        self.stemmer = stemmer\n",
    "        self.lemmatizer=lemmatizer\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.token_map = {}\n",
    "\n",
    "    def word_to_token(self, word):\n",
    "\n",
    "        # Stem to produce a root token\n",
    "        token = self.stemmer.stem(self.lemmatizer.lemmatize(word))\n",
    "\n",
    "        # If the token exists then check the words producing it.\n",
    "        if token in self.token_map:\n",
    "            word_counts = self.token_map[token]\n",
    "    \n",
    "            # If the word already exists then update its count.\n",
    "            if word in word_counts:\n",
    "                word_counts[word] = word_counts[word]+1\n",
    "                \n",
    "            # Otherwise add a new count.\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "    \n",
    "            # Update the token map.\n",
    "            self.token_map[token] = word_counts\n",
    "    \n",
    "        # If there is no token then add a new one with a new word count.\n",
    "        else:\n",
    "            self.token_map[token] = {word: 1}\n",
    "    \n",
    "        return token\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \n",
    "        word_counts = self.token_map[token]\n",
    "        \n",
    "        return sorted(word_counts.keys(), key=lambda key: word_counts[key], reverse=True)[0]\n",
    "\n",
    "    \n",
    "    def words_to_tokens(self, word_list):\n",
    "\n",
    "        return [self.word_to_token(word) for word in word_list]\n",
    "\n",
    "    def tokens_to_words(self, token_list):\n",
    "\n",
    "        return [self.token_to_word(token) for token in token_list]\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(stemmer=LancasterStemmer())\n",
    "(\n",
    "    tokenizer.words_to_tokens(['bias', 'biases', 'moba', 'crs']), \n",
    "    tokenizer.tokens_to_words(tokenizer.words_to_tokens(['bias', 'biases', 'moba', 'crs']))\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(stemmer=PorterStemmer())\n",
    "\n",
    "recsys_topic_model_df['adj_topic_representation'] = (\n",
    "    recsys_topic_model_df['topic_representation']\n",
    "    .map(tokenizer.words_to_tokens)\n",
    "    .map(lambda tokens: sorted(set(tokens), key=tokens.index))\n",
    "    .map(tokenizer.tokens_to_words)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686890e-592b-4638-9a3e-017de8eb1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_topic_name(terms, n=4):\n",
    "    drop_terms = ['tourist', 'recommend', 'recommender', 'recommendation', 'check', 'systems', 'study', 'based', 'contextual', 'tv', 'twitter', 'iptv', 'crs', 'effectiveness', 'explainable']\n",
    "\n",
    "    return ', '.join([term.title() for term in terms if term not in drop_terms][:n])\n",
    "\n",
    "recsys_topic_model_df['adj_topic_name'] = recsys_topic_model_df['adj_topic_representation'].map(improve_topic_name)\n",
    "\n",
    "recsys_topic_model_df['adj_topic_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6aaebd-fb61-45ca-afa4-93255e425d85",
   "metadata": {},
   "source": [
    "# Add topics to papers\n",
    "Next we add the relevant topic information to the main papers dataframes. This means adding topic identifiers and names to to the corresponding paper records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c40a0-ddae-4a71-ac4a-5ba6a951e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_topic_id_by_paper_id = all_topic_model_df['papers'].explode().dropna().reset_index().set_index('papers').add_prefix('universe_')\n",
    "universe_topic_name_by_paper_id = all_topic_model_df.set_index('adj_topic_name')['papers'].explode().dropna().reset_index().set_index('papers').add_prefix('universe_')\n",
    "\n",
    "universe_papers_df_with_topics = (\n",
    "    all_papers_df.set_index('paperId')\n",
    "    .join(universe_topic_id_by_paper_id, how='left')\n",
    "    .join(universe_topic_name_by_paper_id, how='left')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "universe_papers_df_with_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a574ba-80f2-4631-a4b0-b723ca57afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_topic_id_by_paper_id = recsys_topic_model_df['papers'].explode().dropna().reset_index().set_index('papers').add_prefix('recsys_')\n",
    "recsys_topic_name_by_paper_id = recsys_topic_model_df.set_index('adj_topic_name')['papers'].explode().dropna().reset_index().set_index('papers').add_prefix('recsys_')\n",
    "\n",
    "recsys_papers_df_with_topics = (\n",
    "    recsys_papers_df.set_index('paperId')\n",
    "    .join(recsys_topic_id_by_paper_id, how='left')\n",
    "    .join(recsys_topic_name_by_paper_id, how='left')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "recsys_papers_df_with_topics['recsys_adj_topic_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2476b55-72ce-40ea-9e7b-55e2f5efd020",
   "metadata": {},
   "source": [
    "# Save the Updated Datsets\n",
    "These updated datasets will be used in the topic analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d6887-2764-4431-9a37-63d29b6b5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_papers_df_with_topics.to_feather('../data/processed/2410_recsys_papers_with_topics.feather')\n",
    "universe_papers_df_with_topics.to_feather('../data/processed/2410_universe_papers_with_topics.feather')\n",
    "\n",
    "universe_papers_df_with_topics.shape, recsys_papers_df_with_topics.shape, best_recsys_topic_model_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ae879-0e58-4783-be50-b98ac7b0fa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99632ed-5c3e-46e7-a6b6-eeadc7888fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b91d92-3fbd-4438-b85a-c5938b9d6f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c1ff9-9560-4d23-9612-1b14cce1ecef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
