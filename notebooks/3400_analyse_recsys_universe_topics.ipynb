{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a53b6c-4d91-4d67-bf7e-3c3dc31c8f22",
   "metadata": {},
   "source": [
    "# Universe Topic Analysis\n",
    "This notebook looks at teh topic analysis of the universe of RS papers ($U_p$) and produces various analyses and visualisations used in the main study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be619b3d-cb3c-4431-830f-88d501ff7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import Stemmer\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "# Should prevent \"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. \" warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "import string \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "# nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from glob import glob, iglob\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('paper', font_scale=1.25)\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6068b6-2be7-44a4-82e9-f7a5568c2928",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28126265-d449-43a2-bf37-de0172a9dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_df = pd.read_feather('../data/processed/2410_universe_papers_with_topics.feather')\n",
    "recsys_df = pd.read_feather('../data/processed/2410_recsys_papers_with_topics.feather')\n",
    "\n",
    "universe_df.shape, recsys_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377bb598-2c5c-4002-ade6-7c8084a747e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_df_by_id = universe_df.set_index('paperId')\n",
    "recsys_df_by_id = recsys_df.set_index('paperId')\n",
    "\n",
    "len(universe_df_by_id), len(recsys_df_by_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d146e9f-22ea-457e-99de-094c340d5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_ids = set(universe_df_by_id.index)\n",
    "recsys_ids = set(recsys_df_by_id.index)\n",
    "\n",
    "len(universe_ids), len(recsys_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d42bd-841b-42d9-abe7-dc45917e579c",
   "metadata": {},
   "source": [
    "# The Universe of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a22a54-0b72-407c-a406-9519b5c27825",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_universe_topic = universe_df['universe_topic_id'].notnull()\n",
    "\n",
    "topic_universe_df = universe_df[has_universe_topic].copy()\n",
    "topic_universe_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f84606-37f4-457b-a335-8f80bb09524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_all_papers_per_topic = topic_universe_df.groupby('universe_adj_topic_name').size()\n",
    "num_recsys_papers_per_topic = recsys_df.groupby('recsys_adj_topic_name').size()\n",
    "num_non_recsys_papers_per_topic = num_all_papers_per_topic-num_recsys_papers_per_topic\n",
    "\n",
    "topic_universe_paper_ids = set(topic_universe_df['paperId'].unique())\n",
    "recsys_paper_ids = set(recsys_df['paperId'].unique())\n",
    "\n",
    "inside_recsys_paper_ids = set(recsys_df[recsys_df['paper_type']=='inside']['paperId'].unique())\n",
    "outside_recsys_paper_ids = set(recsys_df[recsys_df['paper_type']=='outside']['paperId'].unique())\n",
    "\n",
    "len(topic_universe_paper_ids), len(recsys_paper_ids), len(inside_recsys_paper_ids), len(outside_recsys_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53186207-9022-4bd0-af9f-fc3addf2d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_topics = topic_universe_df.groupby('universe_adj_topic_name').size().sort_values(ascending=False)\n",
    "\n",
    "papers_by_topic = (\n",
    "    topic_universe_df\n",
    "    .groupby(['universe_adj_topic_name', 'is_core_recsys_paper'])\n",
    "    .apply(lambda g: g['paperId'].values)\n",
    "    .unstack()\n",
    "    .applymap(lambda v: v if type(v) is np.ndarray else [])\n",
    "    .rename(columns={False:'non_recsys_papers', True:'recsys_papers'})\n",
    "    .loc[sorted_topics.index]\n",
    ")\n",
    "\n",
    "papers_by_topic['papers'] = topic_universe_df.groupby('universe_adj_topic_name')['paperId'].apply(lambda p: p.values)\n",
    "\n",
    "papers_by_topic['num_papers'] = papers_by_topic['papers'].map(len)\n",
    "papers_by_topic['num_non_recsys_papers'] = papers_by_topic['non_recsys_papers'].map(len)\n",
    "papers_by_topic['num_recsys_papers'] = papers_by_topic['recsys_papers'].map(len)\n",
    "\n",
    "papers_by_topic['total_papers'] = papers_by_topic['num_non_recsys_papers']+papers_by_topic['num_recsys_papers']\n",
    "\n",
    "papers_by_topic['inside_recsys_papers'] = papers_by_topic['recsys_papers'].swifter.apply(\n",
    "    lambda papers: list(set(papers).intersection(inside_recsys_paper_ids))\n",
    ")\n",
    "\n",
    "papers_by_topic['outside_recsys_papers'] = papers_by_topic['recsys_papers'].swifter.apply(\n",
    "    lambda papers: list(set(papers).intersection(outside_recsys_paper_ids))\n",
    ")\n",
    "\n",
    "papers_by_topic['num_inside_recsys_papers'] = papers_by_topic['inside_recsys_papers'].map(len)\n",
    "papers_by_topic['num_outside_recsys_papers'] = papers_by_topic['outside_recsys_papers'].map(len)\n",
    "\n",
    "papers_by_topic['citations'] = (\n",
    "    papers_by_topic['papers']\n",
    "    .swifter\n",
    "    .apply(lambda papers: np.concatenate(universe_df_by_id.loc[papers]['updated_citations'].values))\n",
    ")\n",
    "\n",
    "papers_by_topic['references'] = (\n",
    "    papers_by_topic['papers']\n",
    "    .swifter\n",
    "    .apply(lambda papers: np.concatenate(universe_df_by_id.loc[papers]['references'].values))\n",
    ")\n",
    "\n",
    "papers_by_topic['recsys_citations'] = (\n",
    "    papers_by_topic['citations']\n",
    "    .swifter\n",
    "    .apply(lambda papers: list(set(papers).intersection(recsys_ids)))\n",
    ")\n",
    "\n",
    "papers_by_topic['non_recsys_citations'] = (\n",
    "    papers_by_topic['citations']\n",
    "    .swifter\n",
    "    .apply(lambda papers: list(set(papers).difference(recsys_ids)))\n",
    ")\n",
    "\n",
    "papers_by_topic['recsys_references'] = (\n",
    "    papers_by_topic['references']\n",
    "    .swifter\n",
    "    .apply(lambda papers: list(set(papers).intersection(recsys_ids)))\n",
    ")\n",
    "\n",
    "papers_by_topic['non_recsys_references'] = (\n",
    "    papers_by_topic['references']\n",
    "    .swifter\n",
    "    .apply(lambda papers: list(set(papers).difference(recsys_ids)))\n",
    ")\n",
    "\n",
    "for col in ['citations', 'references', 'recsys_citations', 'non_recsys_citations', 'recsys_references', 'non_recsys_references']:\n",
    "    papers_by_topic['num_'+col] = papers_by_topic[col].map(len)\n",
    "\n",
    "papers_by_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8552c-9fa5-4615-877d-a85e125bda01",
   "metadata": {},
   "source": [
    "## Main Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56f0e2-a564-44e3-b78c-846bd8a86505",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, bx, cx) = plt.subplots(figsize=(12, 13), nrows=3, sharex=True)\n",
    "\n",
    "\n",
    "# The number of non-RS and RS papers by topic.\n",
    "papers_by_topic[['num_non_recsys_papers', 'num_recsys_papers']].plot(ax=ax, kind='bar', stacked=True, ec='k', lw=.5)\n",
    "\n",
    "# Add the number of RecSys papers to each topic bar\n",
    "for x, (_, data) in enumerate(papers_by_topic[['num_non_recsys_papers', 'num_recsys_papers']].iterrows()):\n",
    "    y = data.filter(like='num').sum()\n",
    "    ax.text(x, y, '  {:,}'.format(data['num_recsys_papers']), ha='center', va='bottom', rotation=90, fontsize=10)\n",
    "\n",
    "ax.set_ylim(0, 250000)\n",
    "ax.legend(['Non $R_p$ Papers', '$R_p$ Papers'], ncol=2, frameon=False, loc='upper left')\n",
    "ax.set_title('(a) Number of Papers per Topic')\n",
    "\n",
    "papers_by_topic_without_recsys = papers_by_topic.copy()\n",
    "papers_by_topic_without_recsys.loc['Recommendation, User, Filtering, Items', 'num_inside_recsys_papers'] = 0\n",
    "papers_by_topic_without_recsys.loc['Recommendation, User, Filtering, Items', 'num_outside_recsys_papers'] = 0\n",
    "papers_by_topic_without_recsys.loc['Recommendation, User, Filtering, Items', 'num_recsys_papers'] = 0\n",
    "\n",
    "papers_by_topic_without_recsys['num_recsys_papers'].plot(ax=bx, kind='bar', stacked=True, ec='k', lw=.5, color='tab:orange')\n",
    "\n",
    "bx.set_ylim(0, 3500)\n",
    "bx.legend(['$R_p$ Papers'], ncol=1, frameon=False, loc='upper left')\n",
    "bx.set_title('(b) Number of Core RS Papers ($R_p$) per Topic')\n",
    "\n",
    "# The number of citations to RS papers from a topic and to a topic from RS papers.\n",
    "citations_by_topic_without_recsys = papers_by_topic[['num_recsys_citations', 'num_recsys_references']].copy()\n",
    "citations_by_topic_without_recsys.loc['Recommendation, User, Filtering, Items', 'num_recsys_citations'] = 0\n",
    "citations_by_topic_without_recsys.loc['Recommendation, User, Filtering, Items', 'num_recsys_references'] = 0\n",
    "\n",
    "citations_by_topic_without_recsys[['num_recsys_citations', 'num_recsys_references']].plot(ax=cx, kind='bar', stacked=True, ec='k', lw=.5, color=['tab:olive', 'tab:cyan'])\n",
    "\n",
    "# links_per_paper = citations_by_topic_without_recsys[['num_recsys_citations', 'num_recsys_references']].sum(axis=1)/papers_by_topic['num_papers']\n",
    "\n",
    "# ccx = cx.twinx()\n",
    "# links_per_paper.plot(ax=ccx, lw=.5, c='k', ls='--', marker='x')\n",
    "\n",
    "cx.set_xlabel('')\n",
    "\n",
    "cx.set_ylim(0, 32000)\n",
    "\n",
    "cx.set_xticklabels(papers_by_topic.index, rotation=90)\n",
    "cx.legend(['Citations From $R_p$', 'Citations To $R_p$'], ncol=2, frameon=False, loc='upper left')\n",
    "cx.set_title('(c) Number of Citations To/From Core RS Papers ($R_p$) per Topic.')\n",
    "\n",
    "# ccx.set_ylabel('Citations/Paper')\n",
    "# ccx.set_ylim(0, 1)\n",
    "# ccx.legend(['Citations To/From per Paper'], ncol=1, frameon=False, loc='upper right')\n",
    "\n",
    "cx.set_xlim(-1, len(papers_by_topic))\n",
    "# cx.set_xticklabels([', '.join(label.get_text().split(', ')[:2]) for label in cx.get_xticklabels()])\n",
    "\n",
    "ax.set_ylabel(\"Num Papers ('000s)\")\n",
    "bx.set_ylabel(\"Num RS Papers ('000s)\")\n",
    "cx.set_ylabel(\"Num Citations ('000s)\")\n",
    "\n",
    "bx.set_yticks(range(0, 3001, 1000))\n",
    "\n",
    "for xx in [ax, bx, cx]:\n",
    "    xx.set_yticklabels([int(float(label.get_text())//1000) for label in xx.get_yticklabels()])\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('../graphs/3400_papers_citations_by_topic_abc.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9d20f-4948-4c50-b3d7-9d5719d18d9c",
   "metadata": {},
   "source": [
    "# Topic Wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec3484-0fdf-4d56-b95d-f04e7c5197c9",
   "metadata": {},
   "source": [
    "## The wordcloud grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51762dc3-ff23-4624-ab0f-2856aa4c7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens_by_topic = universe_df.groupby('universe_adj_topic_name').apply(\n",
    "    lambda g: [token \n",
    "               for token in np.unique(np.concatenate(g['reversed_text_tokens'].values))\n",
    "               if (not(token.isdigit())) & (not(bool(re.search(r'\\d', token)))) & (not(token in STOPWORDS))\n",
    "              ]\n",
    ").explode().dropna()\n",
    "\n",
    "\n",
    "unique_tokens_by_topic_value_counts = unique_tokens_by_topic.value_counts()      \n",
    "\n",
    "allowed_tokens = set(unique_tokens_by_topic_value_counts[unique_tokens_by_topic_value_counts.between(3, 39)].index)\n",
    "\n",
    "def draw_wordcloud(ax, papers, col):\n",
    "\n",
    "    text = ' '.join([\n",
    "        word for word in ' '.join(papers[col].values).lower().split()\n",
    "        if word in allowed_tokens\n",
    "    ])\n",
    "    \n",
    "    wc = WordCloud(\n",
    "        width=500, height=500,\n",
    "        min_font_size=10, max_font_size=96,\n",
    "        background_color='white', colormap='twilight',\n",
    "        relative_scaling=0  #Â Use ranks only for scaling\n",
    "        ).generate_from_text(text)\n",
    "        \n",
    "    ax.imshow(wc, interpolation=\"bilinear\")\n",
    "\n",
    "    # ax.axis(\"off\")\n",
    "\n",
    "    ax.set_xlim(-20, 520)\n",
    "    ax.set_ylim(520, -20)\n",
    "    \n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.tick_params('both', length=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8e023-ff09-404d-a126-0f0e7e4b74e1",
   "metadata": {},
   "source": [
    "### Grid 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048eefe6-ec38-4997-b384-f49789d0fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_papers_by_topic = universe_df.groupby(['universe_adj_topic_name']).size().sort_values(ascending=False)\n",
    "\n",
    "ncols = 4\n",
    "# nrows = (len(num_papers_by_topic)//ncols) if len(num_papers_by_topic)%ncols==0 else (len(num_papers_by_topic)//ncols)+1\n",
    "nrows = 6\n",
    "s = 2.5\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(ncols*s, nrows*s), nrows=nrows, ncols=ncols, gridspec_kw=dict(wspace=0, hspace=.3))\n",
    "axs = axs.flatten()\n",
    "\n",
    "universe_papers_by_topic = universe_df.groupby('universe_adj_topic_name')['paperId'].apply(lambda g: g.values)\n",
    "\n",
    "universe_papers_by_id = universe_df.set_index('paperId')\n",
    "\n",
    "for ax, topic_name in zip(axs, num_papers_by_topic.index[:24]):\n",
    "    papers = universe_papers_by_id.loc[universe_papers_by_topic.loc[topic_name]]\n",
    "    \n",
    "    draw_wordcloud(ax, papers, 'title')\n",
    "\n",
    "    # if len(topic_name)>22:\n",
    "    #     title = (', '.join(topic_name.split(', ')[:2]) + '\\n' + ', '.join(topic_name.split(', ')[2:])).title()\n",
    "    # else:\n",
    "    #     title = (topic_name).title()\n",
    "\n",
    "    title = ', '.join(topic_name.split(', ')[:2])\n",
    "\n",
    "    title += '\\n({:,}, {:,})'.format(len(papers), papers['citationCount'].sum())\n",
    "\n",
    "    ax.set_title(title, ha='center')\n",
    "\n",
    "    # Remove the empty graphs.\n",
    "    num_empty = (ncols*nrows)-len(num_papers_by_topic)\n",
    "    if num_empty>0:\n",
    "        for ax in axs[-num_empty:]: ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('../graphs/3400_universe_topics_word_clouds_a.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15d9d5-9813-4f1e-89a2-8d66dac59a7d",
   "metadata": {},
   "source": [
    "### Grid 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fb7fa-e58b-4502-98ac-0a93babeac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "# nrows = (len(num_papers_by_topic)//ncols) if len(num_papers_by_topic)%ncols==0 else (len(num_papers_by_topic)//ncols)+1\n",
    "nrows = 6\n",
    "s = 2.5\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(ncols*s, nrows*s), nrows=nrows, ncols=ncols, gridspec_kw=dict(wspace=0, hspace=.3))\n",
    "axs = axs.flatten()\n",
    "\n",
    "universe_papers_by_topic = universe_df.groupby('universe_adj_topic_name')['paperId'].apply(lambda g: g.values)\n",
    "\n",
    "universe_papers_by_id = universe_df.set_index('paperId')\n",
    "\n",
    "for ax, topic_name in zip(axs, num_papers_by_topic.index[-23:]):\n",
    "    papers = universe_papers_by_id.loc[universe_papers_by_topic.loc[topic_name]]\n",
    "    \n",
    "    draw_wordcloud(ax, papers, 'title')\n",
    "\n",
    "    # if len(topic_name)>22:\n",
    "    #     title = (', '.join(topic_name.split(', ')[:2]) + '\\n' + ', '.join(topic_name.split(', ')[2:])).title()\n",
    "    # else:\n",
    "    #     title = (topic_name).title()\n",
    "\n",
    "    title = ', '.join(topic_name.split(', ')[:2])\n",
    "\n",
    "    title += '\\n({:,}, {:,})'.format(len(papers), papers['citationCount'].sum())\n",
    "\n",
    "    ax.set_title(title, ha='center')\n",
    "\n",
    "    # Remove the empty graphs.\n",
    "    num_empty = (ncols*nrows)-len(num_papers_by_topic.index[-23:])\n",
    "    if num_empty>0:\n",
    "        for ax in axs[-num_empty:]: ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('../graphs/3400_universe_topics_word_clouds_b.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cd5a9-0c47-4000-8d45-ae66708141c4",
   "metadata": {},
   "source": [
    "## Latex Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9cf75e-d197-475a-946c-e9c5f4b60eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pd.concat([\n",
    "        universe_df.groupby('universe_adj_topic_name').size(), \n",
    "        universe_df.groupby('universe_adj_topic_name')['citationCount'].sum(),\n",
    "        universe_df.groupby('universe_adj_topic_name')['citationCount'].sum()/universe_df.groupby('universe_adj_topic_name').size()\n",
    "\n",
    "    ], axis=1).sort_values(by=0, ascending=False).applymap(lambda v: '{:,.0f}'.format(v)).to_latex()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
