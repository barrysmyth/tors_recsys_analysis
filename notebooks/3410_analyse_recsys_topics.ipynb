{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ef673e-40b0-4fb7-b041-3875c4ffa93d",
   "metadata": {},
   "source": [
    "# Analysis of the RS Topics\n",
    "We perform a similar analysis of the RS topics including producing several graphs and wordcloud grids. We also provdie an in-depth analysis and visualisation of the momentum (publication and citation) for each topic over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be619b3d-cb3c-4431-830f-88d501ff7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import Stemmer\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "# Should prevent \"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. \" warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "import string \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "# nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob, iglob\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff2090-ee5e-493d-87cd-ce116a6db1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper', font_scale=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6068b6-2be7-44a4-82e9-f7a5568c2928",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28126265-d449-43a2-bf37-de0172a9dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_df = pd.read_feather('../data/processed/2410_universe_papers_with_topics.feather')\n",
    "recsys_df = pd.read_feather('../data/processed/2410_recsys_papers_with_topics.feather')\n",
    "\n",
    "universe_df.shape, recsys_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b552216-ed41-446d-8244-dbeedf8c40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_df['recsys_adj_topic_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377bb598-2c5c-4002-ade6-7c8084a747e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_df_by_id = universe_df.set_index('paperId')\n",
    "recsys_df_by_id = recsys_df.set_index('paperId')\n",
    "\n",
    "len(universe_df_by_id), len(recsys_df_by_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d146e9f-22ea-457e-99de-094c340d5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_ids = set(universe_df_by_id.index)\n",
    "recsys_ids = set(recsys_df_by_id.index)\n",
    "\n",
    "len(universe_ids), len(recsys_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465e566-edcc-47c4-8bbe-6859a77a8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_recsys_paper_ids = set(recsys_df[recsys_df['paper_type']=='inside']['paperId'].unique())\n",
    "outside_recsys_paper_ids = set(recsys_df[recsys_df['paper_type']=='outside']['paperId'].unique())\n",
    "\n",
    "len(inside_recsys_paper_ids), len(outside_recsys_paper_ids), len(inside_recsys_paper_ids) + len(outside_recsys_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3358d50-f0b1-4dfc-b31e-821d21efcf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_df['recsys_topic_id'].notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171a9dd-ff5a-4915-a181-5bf0b736a24d",
   "metadata": {},
   "source": [
    "# RecSys Topics Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd1df8-80b9-4828-a893-e9f0be0b0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens_by_topic = recsys_df.groupby('recsys_adj_topic_name').apply(\n",
    "    lambda g: [token \n",
    "               for token in np.unique(np.concatenate(g['reversed_text_tokens'].values))\n",
    "               if (not(token.isdigit())) & (not(bool(re.search(r'\\d', token)))) & (not(token in STOPWORDS))\n",
    "              ]\n",
    ").explode().dropna()\n",
    "\n",
    "\n",
    "unique_tokens_by_topic_value_counts = unique_tokens_by_topic.value_counts()      \n",
    "\n",
    "allowed_tokens = set(unique_tokens_by_topic_value_counts[unique_tokens_by_topic_value_counts.between(3, 39)].index)\n",
    "\n",
    "def draw_wordcloud(ax, papers, col):\n",
    "\n",
    "    text = ' '.join([\n",
    "        word for word in ' '.join(papers[col].values).lower().split()\n",
    "        if word in allowed_tokens\n",
    "    ])\n",
    "    \n",
    "    wc = WordCloud(\n",
    "        width=500, height=500,\n",
    "        min_font_size=10, max_font_size=96,\n",
    "        background_color='white', colormap='twilight',\n",
    "        relative_scaling=0  # Use ranks only for scaling\n",
    "        ).generate_from_text(text)\n",
    "        \n",
    "    ax.imshow(wc, interpolation=\"bilinear\")\n",
    "\n",
    "    # ax.axis(\"off\")\n",
    "\n",
    "    ax.set_xlim(-20, 520)\n",
    "    ax.set_ylim(520, -20)\n",
    "    \n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.tick_params('both', length=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a963cb1-1d69-4a87-9910-589a8863b04f",
   "metadata": {},
   "source": [
    "## The Wordcloud Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e89c5-4374-4875-9192-7a0c5788ac36",
   "metadata": {},
   "source": [
    "### Grid 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798da137-240c-4504-8580-c4ea85da7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_papers_by_topic = recsys_df.groupby(['recsys_adj_topic_name']).size().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "ncols = 4\n",
    "# nrows = (len(num_papers_by_topic)//ncols) if len(num_papers_by_topic)%ncols==0 else (len(num_papers_by_topic)//ncols)+1\n",
    "nrows = 6\n",
    "s = 2.5\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(ncols*s, nrows*s), nrows=nrows, ncols=ncols, gridspec_kw=dict(wspace=0, hspace=.3))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "all_papers_by_topic = recsys_df.groupby('recsys_adj_topic_name')['paperId'].apply(lambda g: g.values)\n",
    "\n",
    "recsys_papers_by_id = recsys_df.set_index('paperId')\n",
    "\n",
    "for ax, topic_name in zip(axs, num_papers_by_topic.index[:24]):\n",
    "    papers = recsys_papers_by_id.loc[all_papers_by_topic.loc[topic_name]]\n",
    "\n",
    "    frac_inside = (papers['paper_type']=='inside').sum()/(recsys_df['paper_type']=='inside').sum()\n",
    "    frac_outside = (papers['paper_type']=='outside').sum()/(recsys_df['paper_type']=='outside').sum()\n",
    "    inside_bias = frac_inside/(frac_inside+frac_outside)\n",
    "\n",
    "    \n",
    "    draw_wordcloud(ax, papers, 'title')\n",
    "\n",
    "    \n",
    "    title = ', '.join(topic_name.split(', ')[:2])\n",
    "    if len(title)>18:\n",
    "        title = title[:18]+'...'\n",
    "    \n",
    "\n",
    "    title += '\\n({:,}, {:,}, {:.1f})'.format(len(papers), papers['citationCount'].sum(), inside_bias)\n",
    "\n",
    "    ax.set_title(title, ha='center')\n",
    "\n",
    "    # Remove the empty graphs.\n",
    "    num_empty = (ncols*nrows)-len(num_papers_by_topic)\n",
    "    if num_empty>0:\n",
    "        for ax in axs[-num_empty:]: ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('../graphs/3410_recsys_topics_word_clouds_a.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a955180-191b-4b8a-9367-31b5961f691f",
   "metadata": {},
   "source": [
    "### Grid 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0214d46-4cee-4da6-896c-be4fc9666885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ncols = 4\n",
    "# nrows = (len(num_papers_by_topic)//ncols) if len(num_papers_by_topic)%ncols==0 else (len(num_papers_by_topic)//ncols)+1\n",
    "nrows = 5\n",
    "s = 2.5\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(ncols*s, nrows*s), nrows=nrows, ncols=ncols, gridspec_kw=dict(wspace=0, hspace=.3))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "all_papers_by_topic = recsys_df.groupby('recsys_adj_topic_name')['paperId'].apply(lambda g: g.values)\n",
    "\n",
    "recsys_papers_by_id = recsys_df.set_index('paperId')\n",
    "\n",
    "for ax, topic_name in zip(axs, num_papers_by_topic.index[25:]):\n",
    "    papers = recsys_papers_by_id.loc[all_papers_by_topic.loc[topic_name]]\n",
    "\n",
    "    frac_inside = (papers['paper_type']=='inside').sum()/(recsys_df['paper_type']=='inside').sum()\n",
    "    frac_outside = (papers['paper_type']=='outside').sum()/(recsys_df['paper_type']=='outside').sum()\n",
    "    inside_bias = frac_inside/(frac_inside+frac_outside)\n",
    "\n",
    "    \n",
    "    draw_wordcloud(ax, papers, 'title')\n",
    "\n",
    "\n",
    "    title = ', '.join(topic_name.split(', ')[:2])\n",
    "    if len(title)>18:\n",
    "        title = title[:18]+'...'\n",
    "        \n",
    "    title += '\\n({:,}, {:,}, {:.1f})'.format(len(papers), papers['citationCount'].sum(), inside_bias)\n",
    "\n",
    "    ax.set_title(title, ha='center')\n",
    "\n",
    "    # Remove the empty graphs.\n",
    "    num_empty = (ncols*nrows)-len(num_papers_by_topic[25:])\n",
    "    if num_empty>0:\n",
    "        for ax in axs[-num_empty:]: ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('../graphs/3410_recsys_topics_word_clouds_b.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a78b9-780f-49bf-a5ae-f804dc891d6c",
   "metadata": {},
   "source": [
    "## The Latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ae955-5904-438f-88ef-406ea3380df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pd.concat([\n",
    "        recsys_df.groupby('recsys_adj_topic_name').size(), \n",
    "        recsys_df.groupby('recsys_adj_topic_name')['citationCount'].sum(),\n",
    "        recsys_df.groupby('recsys_adj_topic_name')['citationCount'].sum()/recsys_df.groupby('recsys_adj_topic_name').size()\n",
    "\n",
    "    ], axis=1).sort_values(by=0, ascending=False).applymap(lambda v: '{:,.0f}'.format(v)).to_latex()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd01a2-4d31-41a1-a5f7-8088b7e8a656",
   "metadata": {},
   "source": [
    "# The RecSys Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13757b5-af34-44a9-9f5a-2dc3c65c9dee",
   "metadata": {},
   "source": [
    "## RecSys Topics Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27f963-71e4-4cc3-9797-a71e132775a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_by_topic = pd.DataFrame(all_papers_by_topic)\n",
    "papers_by_topic.columns=['papers']\n",
    "\n",
    "# papers_by_topic['papers'] = recsys_df.groupby('recsys_adj_topic_name')['paperId'].apply(lambda p: p.values)\n",
    "\n",
    "papers_by_topic['num_authors'] = papers_by_topic['papers'].swifter.apply(lambda papers: len(np.unique(np.concatenate(recsys_df_by_id.loc[papers]['authors'].values))))\n",
    "\n",
    "papers_by_topic['inside_papers'] = papers_by_topic['papers'].swifter.apply(\n",
    "    lambda papers: list(set(papers).intersection(inside_recsys_paper_ids))\n",
    ")\n",
    "\n",
    "papers_by_topic['outside_papers'] = papers_by_topic['papers'].swifter.apply(\n",
    "    lambda papers: list(set(papers).intersection(outside_recsys_paper_ids))\n",
    ")\n",
    "\n",
    "for col in ['papers', 'inside_papers', 'outside_papers']:\n",
    "    papers_by_topic['num_' + col] = papers_by_topic[col].map(len)\n",
    "\n",
    "papers_by_topic['frac_inside_papers'] = papers_by_topic['num_inside_papers']/papers_by_topic['num_inside_papers'].sum()\n",
    "papers_by_topic['frac_outside_papers'] = papers_by_topic['num_outside_papers']/papers_by_topic['num_outside_papers'].sum()\n",
    "papers_by_topic['inside_bias'] = papers_by_topic['frac_inside_papers']/(papers_by_topic['frac_inside_papers']+papers_by_topic['frac_outside_papers'])\n",
    "\n",
    "papers_by_topic['num_citations'] = (\n",
    "    papers_by_topic['papers']\n",
    "    .swifter\n",
    "    .apply(lambda papers: universe_df_by_id.loc[papers]['citationCount'].sum())\n",
    ")\n",
    "\n",
    "papers_by_topic['num_inside_citations'] = (\n",
    "    papers_by_topic['inside_papers']\n",
    "    .swifter\n",
    "    .apply(lambda papers: universe_df_by_id.loc[papers]['citationCount'].sum())\n",
    ")\n",
    "\n",
    "papers_by_topic['num_outside_citations'] = (\n",
    "    papers_by_topic['outside_papers']\n",
    "    .swifter\n",
    "    .apply(lambda papers: universe_df_by_id.loc[papers]['citationCount'].sum())\n",
    ")\n",
    "\n",
    "papers_by_topic['num_inside_citations_per_paper'] = (\n",
    "    papers_by_topic['inside_papers']\n",
    "    .swifter\n",
    "    .apply(lambda papers: universe_df_by_id.loc[papers]['citationCount'].values)\n",
    "    .map(np.mean)\n",
    ")\n",
    "\n",
    "papers_by_topic['num_outside_citations_per_paper'] = (\n",
    "    papers_by_topic['outside_papers']\n",
    "    .swifter\n",
    "    .apply(lambda papers: universe_df_by_id.loc[papers]['citationCount'].values)\n",
    "    .map(np.mean)\n",
    ")\n",
    "\n",
    "\n",
    "papers_by_topic['inside_outside_citation_ratio'] = papers_by_topic['num_inside_citations_per_paper']/papers_by_topic['num_outside_citations_per_paper']\n",
    "\n",
    "papers_by_topic = papers_by_topic.sort_values(by='num_papers', ascending=False)\n",
    "\n",
    "papers_by_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c0069-1bd3-45ad-b039-e627ac81431a",
   "metadata": {},
   "source": [
    "## Visualisation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244689d2-9941-43be-b078-f7e955461310",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, bx, cx, dx) = plt.subplots(figsize=(10, 13), nrows=4, sharex=True)\n",
    "\n",
    "\n",
    "\n",
    "# The number of inside/outside papers by topic.\n",
    "papers_by_topic[['num_inside_papers', 'num_outside_papers']].plot(ax=ax, kind='bar', stacked=True, ec='k', lw=.5)\n",
    "\n",
    "ax.set_ylabel('Total Papers')\n",
    "ax.set_ylim(0, 5000)\n",
    "ax.legend(['Inside', 'Outside'], ncol=2, frameon=False, loc='upper left')\n",
    "ax.set_title('(a) Total Inside & Outside Papers per Topic')\n",
    "\n",
    "papers_by_topic[['frac_inside_papers', 'frac_outside_papers']].plot(ax=bx, kind='bar', stacked=True, ec='k', lw=.5)\n",
    "\n",
    "bx.set_ylabel('Frac Papers')\n",
    "bx.set_ylim(0, .15)\n",
    "bx.legend(['Inside', 'Outside'], ncol=2, frameon=False, loc='upper left')\n",
    "bx.set_title('(b) Fraction of Inside & Outside Papers per Topic')\n",
    "\n",
    "bbx = bx.twinx()\n",
    "bbx.scatter(papers_by_topic['inside_bias'].index, papers_by_topic['inside_bias'], lw=.5, c='w',ec='k',  marker='o')\n",
    "\n",
    "bbx.set_ylabel('Inside Bias')\n",
    "bbx.legend(['Inside Bias'], ncol=1, frameon=False, loc='upper right')\n",
    "bbx.axhline(.5, c='k', lw=.5, ls='--')\n",
    "bbx.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "papers_by_topic[['num_inside_citations', 'num_outside_citations']].plot(ax=cx, kind='bar', stacked=True, ec='k', lw=.5)\n",
    "\n",
    "cx.set_ylim(0, 99000)\n",
    "cx.set_xlabel('')\n",
    "cx.set_ylabel('Total Citations')\n",
    "cx.legend(['Inside', 'Outside'], ncol=2, frameon=False, loc='upper left')\n",
    "cx.set_title('(c) Total Inside & Outside Citations per Paper per Topic')\n",
    "\n",
    "papers_by_topic[['num_inside_citations_per_paper', 'num_outside_citations_per_paper']].plot(ax=dx, kind='bar', stacked=True, ec='k', lw=.5)\n",
    "\n",
    "dx.set_xlabel('')\n",
    "dx.set_ylabel('Mean Citations')\n",
    "dx.legend(['Inside', 'Outside'], ncol=2, frameon=False, loc='upper left')\n",
    "dx.set_title('(d) Mean Inside & Outside Citations per Paper per Topic')\n",
    "\n",
    "# ccx = cx.twinx()\n",
    "# ccx.scatter(papers_by_topic['inside_outside_citation_ratio'].index, papers_by_topic['inside_outside_citation_ratio'], lw=.5, c='w',ec='k',  marker='o')\n",
    "# ccx.axhline(1, c='k', lw=.5, ls='--')\n",
    "\n",
    "# ccx.set_ylabel('Inside/Outside Ratio')\n",
    "# ccx.legend(['Inside/Outside Ratio'], ncol=1, frameon=False, loc='upper right')\n",
    "\n",
    "\n",
    "dx.set_xlim(-1, len(papers_by_topic))\n",
    "dx.set_xticklabels([', '.join(label.get_text().split(', ')[:2]) for label in dx.get_xticklabels()])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('../graphs/3410_inside_outside_papers_citations_by_topic_abcd.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c800e-25b2-48c3-b89c-fa353791e782",
   "metadata": {},
   "source": [
    "## Visualisation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5556b-c77c-41be-b6c0-a9561c34042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, bx) = plt.subplots(figsize=(10, 8), nrows=2, sharex=True)\n",
    "\n",
    "\n",
    "# The number of inside/outside papers by topic.\n",
    "papers_by_topic[['num_inside_papers', 'num_outside_papers']].plot(ax=ax, kind='bar', stacked=True, ec='k', lw=.5)\n",
    "\n",
    "ax.set_ylabel('Total Papers')\n",
    "ax.set_ylim(0, 5000)\n",
    "ax.legend(['Inside', 'Outside'], ncol=2, frameon=False, loc='upper left')\n",
    "ax.set_title('(a) Total Inside & Outside Papers per Topic')\n",
    "\n",
    "papers_by_topic[['frac_inside_papers', 'frac_outside_papers']].plot(ax=bx, kind='bar', stacked=True, ec='k', lw=.5)\n",
    "\n",
    "bx.set_ylabel('Frac Papers')\n",
    "bx.set_ylim(0, .15)\n",
    "bx.legend(['Inside', 'Outside'], ncol=2, frameon=False, loc='upper left')\n",
    "bx.set_title('(b) Fraction of Inside & Outside Papers per Topic')\n",
    "\n",
    "bbx = bx.twinx()\n",
    "bbx.scatter(papers_by_topic['inside_bias'].index, papers_by_topic['inside_bias'], lw=.5, c='w',ec='k',  marker='o')\n",
    "\n",
    "bbx.set_ylabel('Inside Bias')\n",
    "bbx.legend(['Inside Bias'], ncol=1, frameon=False, loc='upper right')\n",
    "bbx.axhline(.5, c='k', lw=.5, ls='--')\n",
    "bbx.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "bx.set_xlim(-1, len(papers_by_topic))\n",
    "bx.set_xticklabels([', '.join(label.get_text().split(', ')[:2]) for label in dx.get_xticklabels()])\n",
    "\n",
    "bx.set_xlabel('')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('../graphs/3410_inside_outside_papers_by_topic.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2d34b-c945-4ede-9351-0053131456ca",
   "metadata": {},
   "source": [
    "# Topic Publication/Citation Profiles\n",
    "Here we analyse the publictaion and citation profiles of each of the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d61d9-5e52-4ac8-a64c-0cd82c795785",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_by_topic['paper_years'] = papers_by_topic['papers'].swifter.apply(\n",
    "    lambda papers: sorted([year for year in universe_df_by_id.loc[papers]['year'].values if not(np.isnan(year))])\n",
    ")\n",
    "\n",
    "papers_by_topic['start_year'] = papers_by_topic['paper_years'].map(min)\n",
    "papers_by_topic['end_year'] = papers_by_topic['paper_years'].map(max)\n",
    "\n",
    "papers_by_topic['citation_years'] = papers_by_topic['papers'].swifter.apply(\n",
    "    lambda papers: sorted([year for year in universe_df_by_id.reindex(np.concatenate(universe_df_by_id.loc[papers]['updated_citations'].values))['year'].values if not(np.isnan(year))])\n",
    ")\n",
    "\n",
    "papers_by_topic['total_citation_count'] = papers_by_topic['citation_years'].map(len)\n",
    "\n",
    "papers_by_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915a9c2-d358-4541-8609-abd040e0ae11",
   "metadata": {},
   "source": [
    "## Calculate Momentum (publication output and citation impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e3228-fea1-4b44-b197-d77c4b47355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_window = 4\n",
    "\n",
    "def get_momentum(years, window=momentum_window):\n",
    "\n",
    "    # eg window = 5, end_year = 2023, start_year = 2014, mid_year = 2019, => 10 years including start/end\n",
    "    end_year = max(years)\n",
    "    start_year = (end_year-(window*2))+1\n",
    "    mid_year = start_year+window\n",
    "\n",
    "    # The year at which the topic surpassed 10% of papers or citations\n",
    "    threshold_year = years[len(years)//10]\n",
    "\n",
    "    num_early = len([year for year in years if start_year<=year<mid_year])\n",
    "    num_late = len([year for year in years if mid_year<=year<=end_year])\n",
    "\n",
    "    momentum = (num_late-num_early)/num_early\n",
    "\n",
    "    return pd.Series([start_year, mid_year, end_year, window, num_early, num_late, momentum, threshold_year], \n",
    "                     index=['start_year', 'mid_year', 'end_year', 'window', 'num_early', 'num_late', 'momentum', 'threshold_year'])\n",
    "\n",
    "\n",
    "paper_momentum = papers_by_topic['paper_years'].swifter.apply(get_momentum).add_suffix('_papers')\n",
    "citation_momentum = papers_by_topic['citation_years'].swifter.apply(get_momentum).add_suffix('_citations')\n",
    "\n",
    "papers_by_topic = pd.concat([papers_by_topic, paper_momentum, citation_momentum], axis=1).copy()\n",
    "\n",
    "\n",
    "papers_by_topic.loc['Trust, Social, User, Reputation'].tail(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688f73c-8ae9-4f54-8611-b7fc02f875cf",
   "metadata": {},
   "source": [
    "## Add momemntum information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aef460-8e6e-49eb-863e-d1ed93bc915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_by_topic['momentum'] = ((papers_by_topic['momentum_citations']**2) + (papers_by_topic['momentum_papers']**2)).map(np.sqrt)\n",
    "\n",
    "papers_by_topic['growing_papers'] = papers_by_topic['momentum_papers']>0\n",
    "papers_by_topic['growing_citations'] = papers_by_topic['momentum_citations']>0\n",
    "\n",
    "papers_by_topic[(papers_by_topic['growing_papers']) & (papers_by_topic['growing_citations'])].sort_values(by='momentum', ascending=False).loc[:, 'num_early_papers':]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deafa03-bd26-49f6-ac2e-f80e76baccc4",
   "metadata": {},
   "source": [
    "## Main Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864215b-e392-43de-8426-1e1cbe97b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_momentum_label(momentum):\n",
    "\n",
    "    threshold = 0.05\n",
    "\n",
    "    if momentum>threshold:\n",
    "        return '↑'\n",
    "    elif -threshold<=momentum<=threshold:\n",
    "        return ' ↔︎'\n",
    "    elif momentum<-threshold:\n",
    "        return '↓'\n",
    "            \n",
    "def plot_topic_publication_sparklines(ax, topics_df):\n",
    "    \n",
    "    for i, (topic_id, topic_data) in enumerate(topics_df.iterrows()):\n",
    "\n",
    "        start_year, end_year = topic_data['start_year'], topic_data['end_year']\n",
    "        \n",
    "        scale = 0.8\n",
    "\n",
    "        # We start the profiles only once we exceed 5% of the data.\n",
    "        # This avoids some issues with premature papers/cites.\n",
    "        profile_threshold = 0.05\n",
    "        \n",
    "        pub_profile = pd.Series(topic_data['paper_years']).value_counts().sort_index().loc[start_year:end_year]\n",
    "        rel_pub_profile = (pub_profile/pub_profile.max())\n",
    "        rel_pub_profile = rel_pub_profile.loc[rel_pub_profile.cumsum()[rel_pub_profile.cumsum()>profile_threshold].index]*scale\n",
    "\n",
    "        cite_profile = pd.Series(topic_data['citation_years']).value_counts().sort_index().loc[start_year:end_year]\n",
    "        rel_cite_profile = (cite_profile/cite_profile.max())\n",
    "        rel_cite_profile = rel_cite_profile.loc[rel_cite_profile.cumsum()[rel_cite_profile.cumsum()>profile_threshold].index]*scale\n",
    "\n",
    "\n",
    "        # The colour of the graph is based on the inside bias.\n",
    "        inside_bias = topic_data['inside_bias']\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "        cmap = plt.get_cmap('coolwarm_r') \n",
    "        c = cmap(norm(inside_bias))\n",
    "\n",
    "        # Draw and fill the graph\n",
    "        ax.plot(rel_pub_profile.index, i+rel_pub_profile.values, lw=1.5, ls='-', c=c)\n",
    "        ax.fill_between(rel_pub_profile.index, i, i+rel_pub_profile, color=c, alpha=.5)\n",
    "        # (i+profile).plot(ax=ax, lw=.75, c='k')\n",
    "\n",
    "        ax.plot(rel_cite_profile.index, i+rel_cite_profile.values, lw=.6, ls='-', c='k')\n",
    "\n",
    "        # ax.plot([topic_data['start_year_papers']]*2, [i, i+scale], lw=.5, ls=':', c='k')\n",
    "        # ax.plot([topic_data['mid_year_papers']]*2, [i, i+scale], lw=.75, ls='-', c='k')\n",
    "\n",
    "        threshold_year = topic_data['threshold_year_papers']\n",
    "        # ax.plot([threshold_year]*2, [i, i+rel_pub_profile.loc[threshold_year]], lw=.5, c='k')\n",
    "        \n",
    "        # ax.plot([topic_data['end_year_papers']]*2, [i, i+scale], lw=.5, ls=':', c='k')\n",
    "\n",
    "        num_authors = topic_data['num_authors']\n",
    "        topic_count = topic_data['num_papers']\n",
    "        total_citation_count = topic_data['total_citation_count']\n",
    "        pub_momentum_label = get_momentum_label(topic_data['momentum_papers'])\n",
    "        cite_momentum_label = get_momentum_label(topic_data['momentum_citations'])\n",
    "        cites_per_paper = total_citation_count/topic_count\n",
    "\n",
    "        # Add the total topic count and citation count and the momentums.\n",
    "        # ax.text(rel_pub_profile.index[0], i, '{:,.0f}{}, {:,.0f}{}'.format(\n",
    "        #     topic_count, pub_momentum_label, total_citation_count, cite_momentum_label\n",
    "        # ), ha='right', va='bottom')\n",
    "\n",
    "        ax.text(rel_pub_profile.index[0], i, '{:,.0f} papers'.format(\n",
    "                topic_count, total_citation_count\n",
    "            ), ha='right', va='bottom')\n",
    "\n",
    "    ax.set_ylim(0, len(topics_df))\n",
    "    ax.set_yticks(range(0, len(topics_df)))\n",
    "     \n",
    "    y_labels = topics_df.apply(\n",
    "        lambda topic: '{} ({:.1f}%, {:.1f}%)'.format(', '.join(topic.name.split(', ')[:2]), topic['frac_inside_papers']*100, topic['frac_outside_papers']*100), axis=1\n",
    "    )\n",
    "    ax.set_yticklabels(y_labels, va='bottom')\n",
    "    \n",
    "    ax.tick_params(axis='y', pad=-5)\n",
    "    \n",
    "    # cax = inset_axes(ax, width=\"5%\", height=\"30%\", loc='lower left') \n",
    "    \n",
    "    # cmappable = mpl.cm.ScalarMappable(cmap=cmap)\n",
    "    # cb = fig.colorbar(cmappable, cax=cax, orientation='vertical', location='left')\n",
    "    # cb.set_ticklabels(['100%\\nOutside', '', '', '', '', '100%\\nInside'], ha='right')\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "    cmap = plt.get_cmap('coolwarm_r') \n",
    "    \n",
    "    sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cax = fig.add_axes([.93, 0.03, 0.02, .25])\n",
    "    cbar = plt.colorbar(sm, cax=cax, alpha=.5)\n",
    "    cbar.set_label('Inside Bias')\n",
    "\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    # ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.yaxis.set_tick_params(which='major', right=False)\n",
    "\n",
    "    # ax.axvline(2007, lw=.5, ls=':', c='k', zorder=-100)\n",
    "    \n",
    "    ax.set_xlim(1990, 2023.2)\n",
    "    ax.set_xticks(range(1990, 2023, 10))\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "    # ax.axvline(2020, c='k', ls='--', lw=.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005de4e-40d0-4a5a-9d28-405940ba3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = 10, 13\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(w, h))\n",
    "\n",
    "plot_topic_publication_sparklines(ax, papers_by_topic.sort_values(by='num_papers'))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('../graphs/3410_recsys_topic_profiles.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4034a42-0658-45e4-8dd9-546a98aafb2c",
   "metadata": {},
   "source": [
    "# Save Updated DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca1ee0-2b36-41c9-b371-22beec3157c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_by_topic.to_feather('../data/processed/3410_papers_by_topic.feather')\n",
    "papers_by_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caef6e7-7d57-47df-9131-a57934b4c64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a397e63-ec2b-4118-b367-f48250b9161d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ea153-45f3-426c-ad64-37d6cc7d0451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016f44e-b86f-4c82-935a-022a10b9c178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcbf59d-b5c3-4c03-8704-7799de2550e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab5109-afc5-4fff-b4b8-15925c5170b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
